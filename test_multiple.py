from scraper import CoinDeskScraper
import time


def test_multiple_articles():
    """–¢–µ—Å—Ç–≤–∞ scraping –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–∏–∏"""
    print("=== –¢–ï–°–¢ –ù–ê –ú–ù–û–ñ–ï–°–¢–í–û –°–¢–ê–¢–ò–ò ===")

    scraper = CoinDeskScraper()

    # –ü—ä—Ä–≤–æ –≤–∑–µ–º–∞–º–µ –ª–∏–Ω–∫–æ–≤–µ—Ç–µ
    print("üîç –ù–∞–º–∏—Ä–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏–∏...")
    article_links = scraper.get_article_links()

    print(f"üì∞ –ù–∞–º–µ—Ä–µ–Ω–∏ {len(article_links)} —Å—Ç–∞—Ç–∏–∏")
    print("\nüìã –ü—ä—Ä–≤–∏ 10 —Å—Ç–∞—Ç–∏–∏:")
    for i, link in enumerate(article_links[:10], 1):
        title = link['title'][:60] + "..." if len(link['title']) > 60 else link['title']
        print(f"  {i}. {title}")
        print(f"     ‚Üí {link['href']}")

    # Scrape-–≤–∞–º–µ –ø—ä—Ä–≤–∏—Ç–µ 3 —Å—Ç–∞—Ç–∏–∏ –∑–∞ —Ç–µ—Å—Ç
    print(f"\nüéØ –¢–µ—Å—Ç–≤–∞–º scraping –Ω–∞ –ø—ä—Ä–≤–∏—Ç–µ 3 —Å—Ç–∞—Ç–∏–∏...")

    scraped_articles = scraper.scrape_multiple_articles(max_articles=3)

    print(f"\nüìä –†–ï–ó–£–õ–¢–ê–¢–ò:")
    print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏: {len(scraped_articles)} —Å—Ç–∞—Ç–∏–∏")

    # –ü–æ–∫–∞–∑–≤–∞–º–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ
    for i, article in enumerate(scraped_articles, 1):
        print(f"\nüìÑ –°—Ç–∞—Ç–∏—è {i}:")
        print(f"   üì∞ –ó–∞–≥–ª–∞–≤–∏–µ: {article['title']}")
        print(f"   üìÖ –î–∞—Ç–∞: {article['date']}")
        print(f"   üë§ –ê–≤—Ç–æ—Ä: {article['author']}")
        print(f"   üìä –î—ä–ª–∂–∏–Ω–∞: {article['content_length']} —Å–∏–º–≤–æ–ªa")
        print(f"   üîó URL: {article['url']}")
        print(f"   üìù –ü—ä—Ä–≤–∏ 150 —Å–∏–º–≤–æ–ª–∞: {article['content'][:150]}...")

    return scraped_articles


def test_link_validation():
    """–¢–µ—Å—Ç–≤–∞ URL validation –ª–æ–≥–∏–∫–∞—Ç–∞"""
    print("\n=== –¢–ï–°–¢ –ù–ê URL VALIDATION ===")

    scraper = CoinDeskScraper()

    # –í–∑–µ–º–∞–º–µ –≤—Å–∏—á–∫–∏ –ª–∏–Ω–∫–æ–≤–µ
    article_links = scraper.get_article_links()

    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:")
    print(f"   üì∞ –í–∞–ª–∏–¥–Ω–∏ —Å—Ç–∞—Ç–∏–∏: {len(article_links)}")

    # –ü–æ–∫–∞–∑–≤–∞–º–µ —Ä–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ—Ç–æ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    categories = {}
    for link in article_links:
        href = link['href']
        if '/markets/' in href:
            categories['markets'] = categories.get('markets', 0) + 1
        elif '/policy/' in href:
            categories['policy'] = categories.get('policy', 0) + 1
        elif '/tech/' in href:
            categories['tech'] = categories.get('tech', 0) + 1
        elif '/business/' in href:
            categories['business'] = categories.get('business', 0) + 1
        else:
            categories['other'] = categories.get('other', 0) + 1

    print(f"   üìà –†–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
    for category, count in categories.items():
        print(f"      - {category}: {count} —Å—Ç–∞—Ç–∏–∏")


if __name__ == "__main__":
    start_time = time.time()

    # –¢–µ—Å—Ç 1: URL validation
    test_link_validation()

    # –¢–µ—Å—Ç 2: Scraping –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–∏–∏
    articles = test_multiple_articles()

    total_time = time.time() - start_time

    print(f"\nüïí –û–±—â–æ –≤—Ä–µ–º–µ: {total_time:.1f} —Å–µ–∫—É–Ω–¥–∏")
    print(f"üéâ –¢–µ—Å—Ç—ä—Ç –∑–∞–≤—ä—Ä—à–∏ —É—Å–ø–µ—à–Ω–æ!")

    if articles:
        print(f"üíæ –ì–æ—Ç–æ–≤ –∑–∞ –¥–æ–±–∞–≤—è–Ω–µ –Ω–∞ database —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç!")
