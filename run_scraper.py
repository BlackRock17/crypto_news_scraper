#!/usr/bin/env python3
"""
CoinDesk Crypto News Scraper - Command Line Interface Ñ Smart Scraping
Ğ˜Ğ·Ğ¿Ğ¾Ğ»Ğ·Ğ²Ğ°Ğ½Ğµ: python run_scraper.py [Ğ¾Ğ¿Ñ†Ğ¸Ğ¸]
"""

import argparse
import time
import sys
import re
from datetime import datetime, timedelta
from pathlib import Path

# Import Ğ·Ğ° Ğ½Ğ¾Ğ²Ğ¸Ñ scraper
try:
    from improved_latest_news_scraper import CoinDeskLatestNewsScraper

    LATEST_NEWS_AVAILABLE = True
except ImportError:
    LATEST_NEWS_AVAILABLE = False

# Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¸ imports
from scraper import CoinDeskScraper
from postgres_database import PostgreSQLDatabaseManager as DatabaseManager


def scrape_command(args):
    """ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ·Ğ° scraping Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸ (Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ½Ğ°)"""
    print("=== COINDESK CRYPTO NEWS SCRAPER ===")
    print(f"ğŸ¯ Scraping Ğ½Ğ° Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ {args.limit} ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸...")

    scraper = CoinDeskScraper(use_database=True)

    if args.verbose:
        print("\nğŸ“Š ĞĞ°Ñ‡Ğ°Ğ»Ğ½Ğ¸ database ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸:")
        stats = scraper.db.get_database_stats()
        for key, value in stats.items():
            print(f"   {key}: {value}")

    start_time = time.time()
    articles = scraper.scrape_multiple_articles(max_articles=args.limit, save_to_db=True)
    scrape_time = time.time() - start_time

    print(f"\nğŸ“ˆ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¢ĞĞ¢Ğ˜:")
    print(f"   âœ… ĞĞ¾Ğ²Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸: {len(articles)}")
    print(f"   ğŸ•’ Ğ’Ñ€ĞµĞ¼Ğµ: {scrape_time:.1f} ÑĞµĞºÑƒĞ½Ğ´Ğ¸")

    stats = scraper.db.get_database_stats()
    print(f"   ğŸ“Š ĞĞ±Ñ‰Ğ¾ Ğ² Ğ±Ğ°Ğ·Ğ°: {stats['total_articles']} ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸")
    print(f"   ğŸ“‹ Ğ—Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·: {stats['unprocessed_articles']} ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸")

    if len(articles) > 0:
        print(f"\nğŸ“° ĞĞ°Ğ¹-Ğ½Ğ¾Ğ²Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸:")
        for i, article in enumerate(articles[:3], 1):
            print(f"   {i}. {article['title'][:60]}...")

    return len(articles)


def scrape_smart_command(args):
    """Smart scraping Ñ date filtering"""
    if not LATEST_NEWS_AVAILABLE:
        print("âŒ Smart scraping Ğ½Ğµ Ğµ Ğ´Ğ¾ÑÑ‚ÑŠĞ¿ĞµĞ½. ĞœĞ¾Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸ improved_latest_news_scraper.py")
        return False

    print("=== SMART COINDESK SCRAPER ===")
    print(f"ğŸ¯ Smart scraping: {args.limit} ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸, Ñ„Ğ¸Ğ»Ñ‚ÑŠÑ€: {args.date_filter}")

    scraper = CoinDeskLatestNewsScraper(use_database=True)

    if args.verbose:
        print("\nğŸ“Š ĞĞ°Ñ‡Ğ°Ğ»Ğ½Ğ¸ database ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸:")
        stats = scraper.db.get_database_stats()
        for key, value in stats.items():
            print(f"   {key}: {value}")

    start_time = time.time()
    articles = scraper.scrape_articles_smart(
        date_filter=args.date_filter,
        limit=args.limit,
        save_to_db=True
    )
    scrape_time = time.time() - start_time

    print(f"\nğŸ“ˆ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¢ĞĞ¢Ğ˜:")
    print(f"   âœ… ĞĞ¾Ğ²Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸: {len(articles)}")
    print(f"   ğŸ•’ Ğ’Ñ€ĞµĞ¼Ğµ: {scrape_time:.1f} ÑĞµĞºÑƒĞ½Ğ´Ğ¸")

    if scraper.db:
        stats = scraper.db.get_database_stats()
        print(f"   ğŸ“Š ĞĞ±Ñ‰Ğ¾ Ğ² Ğ±Ğ°Ğ·Ğ°: {stats['total_articles']} ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸")
        print(f"   ğŸ“‹ Ğ—Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·: {stats['unprocessed_articles']} ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸")

    if len(articles) > 0:
        print(f"\nğŸ“° Scraped ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸:")
        for i, article in enumerate(articles[:5], 1):
            print(f"   {i}. {article['title'][:60]}...")

    return len(articles)


def status_command(args):
    """ĞŸĞ¾ĞºĞ°Ğ·Ğ²Ğ° database ÑÑ‚Ğ°Ñ‚ÑƒÑ"""
    print("=== DATABASE Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡ ===")

    db = DatabaseManager()
    stats = db.get_database_stats()

    print(f"ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸:")
    print(f"   ğŸ“° ĞĞ±Ñ‰Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸: {stats['total_articles']}")
    print(f"   ğŸ“‹ ĞĞµĞ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ¸: {stats['unprocessed_articles']}")
    print(f"   âœ… ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ¸: {stats['analyzed_articles']}")
    print(f"   ğŸ”— Scraped URLs: {stats['total_scraped_urls']}")

    if stats['latest_article']:
        title, date = stats['latest_article']
        print(f"   ğŸ“… ĞĞ°Ğ¹-Ğ½Ğ¾Ğ²Ğ°: {title[:50]}... ({date})")

    if args.verbose and stats['unprocessed_articles'] > 0:
        print(f"\nğŸ“‹ ĞĞµĞ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸:")
        unprocessed = db.get_unprocessed_articles(limit=5)
        for i, article in enumerate(unprocessed, 1):
            print(f"   {i}. {article['title'][:60]}...")
            print(f"      ğŸ“Š {article['content_length']} chars | {article['scraped_at']}")


def date_status_command(args):
    """Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğ°"""
    print("=== Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡ ĞŸĞ Ğ”ĞĞ¢Ğ ===")

    if not LATEST_NEWS_AVAILABLE:
        print("âš ï¸ Smart Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğµ ÑĞ° Ğ´Ğ¾ÑÑ‚ÑŠĞ¿Ğ½Ğ¸")
        return

    scraper = CoinDeskLatestNewsScraper(use_database=True)

    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞ¼Ğµ Ğ´Ğ°Ñ‚Ğ¸ Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°
    dates_to_check = []
    if args.date == 'today':
        dates_to_check = [datetime.now().strftime('%Y-%m-%d')]
    elif args.date == 'yesterday':
        yesterday = datetime.now() - timedelta(days=1)
        dates_to_check = [yesterday.strftime('%Y-%m-%d')]
    elif args.date == 'week':
        for i in range(7):
            date = datetime.now() - timedelta(days=i)
            dates_to_check.append(date.strftime('%Y-%m-%d'))
    elif re.match(r'\d{4}-\d{2}-\d{2}', args.date):
        dates_to_check = [args.date]
    else:
        for i in range(3):
            date = datetime.now() - timedelta(days=i)
            dates_to_check.append(date.strftime('%Y-%m-%d'))

    print(f"ğŸ“… ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞ²Ğ°Ğ½Ğµ Ğ½Ğ° {len(dates_to_check)} Ğ´Ğ°Ñ‚Ğ¸...")

    total_scraped = 0
    total_new = 0

    for date_str in dates_to_check:
        status = scraper.get_scraping_status_by_date(date_str)

        if 'error' in status:
            print(f"âŒ {date_str}: Ğ“Ñ€ĞµÑˆĞºĞ°")
            continue

        # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ°Ğ½Ğµ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°Ñ‚Ğ°
        if date_str == datetime.now().strftime('%Y-%m-%d'):
            day_label = "Ğ´Ğ½ĞµÑ"
        elif date_str == (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'):
            day_label = "Ğ²Ñ‡ĞµÑ€Ğ°"
        else:
            day_label = ""

        print(f"ğŸ“… {date_str} ({day_label}):")
        print(f"   ğŸ“° Scraped: {status['scraped_articles']}")
        print(f"   ğŸ†• ĞĞ¾Ğ²Ğ¸: {status['new_to_scrape']}")

        total_scraped += status['scraped_articles']
        total_new += status['new_to_scrape']

    print(f"\nğŸ“Š ĞĞ‘ĞĞ‘Ğ©Ğ•ĞĞ˜Ğ•:")
    print(f"   ğŸ“° ĞĞ±Ñ‰Ğ¾ scraped: {total_scraped}")
    print(f"   ğŸ†• ĞĞ±Ñ‰Ğ¾ Ğ½Ğ¾Ğ²Ğ¸: {total_new}")

    if total_new > 0:
        print(f"\nğŸ’¡ ĞŸĞ Ğ•ĞŸĞĞ ĞªĞšĞ:")
        limit = min(total_new, 15)
        print(f"   ğŸ¯ python run_scraper.py scrape-smart --date today --limit {limit}")


def recommend_scraping_command(args):
    """ĞŸÑ€ĞµĞ¿Ğ¾Ñ€ÑŠĞºĞ¸ Ğ·Ğ° scraping"""
    print("=== ĞŸĞ Ğ•ĞŸĞĞ ĞªĞšĞ˜ Ğ—Ğ SCRAPING ===")

    db = DatabaseManager()
    today = datetime.now().strftime('%Y-%m-%d')

    try:
        import sqlite3
        conn = sqlite3.connect(db.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT COUNT(*) FROM articles 
            WHERE url LIKE ? OR published_date = ?
        """, (f'%{today.replace("-", "/")}/%', today))

        today_count = cursor.fetchone()[0]

        print(f"ğŸ“… Ğ”Ğ½ĞµÑ Ğµ: {today}")
        print(f"ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ´Ğ½ĞµÑĞºĞ°: {today_count}")

        print(f"\nğŸ’¡ ĞŸĞ Ğ•ĞŸĞĞ ĞªĞšĞ˜:")

        if LATEST_NEWS_AVAILABLE:
            if today_count == 0:
                print("   ğŸ¯ python run_scraper.py scrape-smart --date today --limit 15")
            elif today_count < 5:
                print("   ğŸ¯ python run_scraper.py scrape-smart --date today --limit 10")
            else:
                print("   âœ… Ğ”Ğ¾Ğ±Ñ€Ğ¾ Ğ¿Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸Ğµ Ğ·Ğ° Ğ´Ğ½ĞµÑĞºĞ°")

            print("\nğŸš€ ĞĞĞ’Ğ˜ Ğ’ĞªĞ—ĞœĞĞ–ĞĞĞ¡Ğ¢Ğ˜:")
            print("   ğŸ“… Ğ’Ñ‡ĞµÑ€Ğ°: python run_scraper.py scrape-smart --date yesterday --limit 10")
            print("   ğŸ“… Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: python run_scraper.py date-status --date week")
        else:
            if today_count == 0:
                print("   ğŸ¯ python run_scraper.py scrape --limit 15")
            else:
                print("   ğŸ¯ python run_scraper.py scrape --limit 10")

        conn.close()

    except Exception as e:
        print(f"âŒ Ğ“Ñ€ĞµÑˆĞºĞ°: {e}")


def export_command(args):
    """Export Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ğ¸"""
    print("=== EXPORT ĞĞ Ğ”ĞĞĞĞ˜ ===")
    db = DatabaseManager()

    if args.all:
        count = db.export_articles_to_json(args.output)
        print(f"ğŸ“¤ Ğ•ĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ°Ğ½Ğ¸ {count} ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸ Ğ² {args.output}")
    else:
        count = db.export_articles_to_json(args.output, processed_only=False)
        print(f"ğŸ“¤ Ğ•ĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ°Ğ½Ğ¸ {count} Ğ½ĞµĞ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸ Ğ² {args.output}")


def cleanup_command(args):
    """Cleanup Ğ½Ğ° ÑÑ‚Ğ°Ñ€Ğ¸ Ğ´Ğ°Ğ½Ğ½Ğ¸"""
    print("=== CLEANUP ĞĞ Ğ¡Ğ¢ĞĞ Ğ˜ Ğ”ĞĞĞĞ˜ ===")
    db = DatabaseManager()

    if not args.dry_run:
        deleted_count = db.cleanup_old_analyzed_articles(days_to_keep=args.days)
        print(f"ğŸ§¹ Ğ˜Ğ·Ñ‚Ñ€Ğ¸Ñ‚Ğ¸ {deleted_count} Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸ (Ğ¿Ğ¾-ÑÑ‚Ğ°Ñ€Ğ¸ Ğ¾Ñ‚ {args.days} Ğ´Ğ½Ğ¸)")
    else:
        print(f"ğŸ” DRY RUN: Ğ‘Ğ¸ Ğ¸Ğ·Ñ‚Ñ€Ğ¸Ğ» Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸ Ğ¿Ğ¾-ÑÑ‚Ğ°Ñ€Ğ¸ Ğ¾Ñ‚ {args.days} Ğ´Ğ½Ğ¸")


def analyze_command(args):
    """ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ²Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸ Ğ·Ğ° sentiment Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·"""
    print("=== Ğ˜ĞĞ¢Ğ•Ğ“Ğ ĞĞ¦Ğ˜Ğ¯ Ğ¡ĞªĞ¡ SENTIMENT ANALYZER ===")
    db = DatabaseManager()

    unprocessed = db.get_unprocessed_articles(limit=args.limit)

    if not unprocessed:
        print("ğŸ“‹ ĞÑĞ¼Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸ Ğ·Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·")
        return

    articles_for_analysis = []
    for article in unprocessed:
        articles_for_analysis.append({
            'id': article['id'],
            'title': article['title'],
            'content': article['content'],
            'url': article['url']
        })

    import json
    with open('articles_for_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(articles_for_analysis, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“¤ Ğ•ĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ°Ğ½Ğ¸ {len(articles_for_analysis)} ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸ Ğ² articles_for_analysis.json")


def mark_analyzed_command(args):
    """ĞœĞ°Ñ€ĞºĞ¸Ñ€Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸ ĞºĞ°Ñ‚Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ¸"""
    print("=== ĞœĞĞ ĞšĞ˜Ğ ĞĞĞ• ĞĞ Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ˜ ĞšĞĞ¢Ğ ĞĞĞĞ›Ğ˜Ğ—Ğ˜Ğ ĞĞĞ˜ ===")
    db = DatabaseManager()

    if args.article_id:
        db.mark_article_as_analyzed(args.article_id)
        print(f"âœ… Ğ¡Ñ‚Ğ°Ñ‚Ğ¸Ñ {args.article_id} Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ°Ğ½Ğ° ĞºĞ°Ñ‚Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ°")
    elif args.all_processed:
        unprocessed = db.get_unprocessed_articles()
        for article in unprocessed:
            db.mark_article_as_analyzed(article['id'])
        print(f"âœ… ĞœĞ°Ñ€ĞºĞ¸Ñ€Ğ°Ğ½Ğ¸ {len(unprocessed)} ÑÑ‚Ğ°Ñ‚Ğ¸Ğ¸ ĞºĞ°Ñ‚Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ¸")


def main():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ"""
    parser = argparse.ArgumentParser(
        description="CoinDesk Crypto News Scraper Ñ Smart Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»Ğ½Ğ¾ÑÑ‚",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ğ¸:

ĞšĞ›ĞĞ¡Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜:
  python run_scraper.py scrape --limit 10
  python run_scraper.py status --verbose

SMART SCRAPING:
  python run_scraper.py scrape-smart --date today --limit 10
  python run_scraper.py scrape-smart --date yesterday --limit 15
  python run_scraper.py scrape-smart --date 2025-06-09 --limit 20

Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡:
  python run_scraper.py date-status --date today
  python run_scraper.py recommend
        """
    )

    subparsers = parser.add_subparsers(dest='command', help='ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¸')

    # Scrape
    scrape_parser = subparsers.add_parser('scrape', help='ĞšĞ»Ğ°ÑĞ¸Ñ‡ĞµÑĞºĞ¸ scraping')
    scrape_parser.add_argument('--limit', type=int, default=10)
    scrape_parser.add_argument('--verbose', action='store_true')

    # Smart scrape
    if LATEST_NEWS_AVAILABLE:
        smart_parser = subparsers.add_parser('scrape-smart', help='Smart scraping')
        smart_parser.add_argument('--date', dest='date_filter', default='today')
        smart_parser.add_argument('--limit', type=int, default=10)
        smart_parser.add_argument('--verbose', action='store_true')

    # Status
    status_parser = subparsers.add_parser('status', help='Database ÑÑ‚Ğ°Ñ‚ÑƒÑ')
    status_parser.add_argument('--verbose', action='store_true')

    # Date status
    if LATEST_NEWS_AVAILABLE:
        date_status_parser = subparsers.add_parser('date-status', help='Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğ°')
        date_status_parser.add_argument('--date', default='today')

    # Recommend
    recommend_parser = subparsers.add_parser('recommend', help='ĞŸÑ€ĞµĞ¿Ğ¾Ñ€ÑŠĞºĞ¸')

    # Export
    export_parser = subparsers.add_parser('export', help='Ğ•ĞºÑĞ¿Ğ¾Ñ€Ñ‚')
    export_parser.add_argument('--output', default='articles.json')
    export_parser.add_argument('--all', action='store_true')

    # Cleanup
    cleanup_parser = subparsers.add_parser('cleanup', help='Cleanup')
    cleanup_parser.add_argument('--days', type=int, default=7)
    cleanup_parser.add_argument('--dry-run', action='store_true')

    # Analyze
    analyze_parser = subparsers.add_parser('analyze', help='Ğ—Ğ° sentiment Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·')
    analyze_parser.add_argument('--limit', type=int, default=5)

    # Mark analyzed
    mark_parser = subparsers.add_parser('mark_analyzed', help='ĞœĞ°Ñ€ĞºĞ¸Ñ€Ğ°Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ¸')
    mark_parser.add_argument('--article-id', type=int)
    mark_parser.add_argument('--all-processed', action='store_true')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    try:
        if args.command == 'scrape':
            scrape_command(args)
        elif args.command == 'scrape-smart' and LATEST_NEWS_AVAILABLE:
            scrape_smart_command(args)
        elif args.command == 'status':
            status_command(args)
        elif args.command == 'date-status' and LATEST_NEWS_AVAILABLE:
            date_status_command(args)
        elif args.command == 'recommend':
            recommend_scraping_command(args)
        elif args.command == 'export':
            export_command(args)
        elif args.command == 'cleanup':
            cleanup_command(args)
        elif args.command == 'analyze':
            analyze_command(args)
        elif args.command == 'mark_analyzed':
            mark_analyzed_command(args)
        else:
            print(f"âŒ ĞĞµÑ€Ğ°Ğ·Ğ¿Ğ¾Ğ·Ğ½Ğ°Ñ‚Ğ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°: {args.command}")
            if not LATEST_NEWS_AVAILABLE:
                print("ğŸ’¡ Smart Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğµ ÑĞ° Ğ´Ğ¾ÑÑ‚ÑŠĞ¿Ğ½Ğ¸")

        print(f"\nâœ… ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° '{args.command}' Ğ·Ğ°Ğ²ÑŠÑ€ÑˆĞµĞ½Ğ°!")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ĞŸÑ€ĞµĞºÑ€Ğ°Ñ‚ĞµĞ½Ğ¾")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Ğ“Ñ€ĞµÑˆĞºĞ°: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
