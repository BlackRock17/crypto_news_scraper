#!/usr/bin/env python3
"""
CoinDesk Crypto News Scraper - Command Line Interface
–ò–∑–ø–æ–ª–∑–≤–∞–Ω–µ: python run_scraper.py [–æ–ø—Ü–∏–∏]
"""

import argparse
import time
import sys
from pathlib import Path

from scraper import CoinDeskScraper
from database import DatabaseManager


def scrape_command(args):
    """–ö–æ–º–∞–Ω–¥–∞ –∑–∞ scraping –Ω–∞ –Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏"""
    print("=== COINDESK CRYPTO NEWS SCRAPER ===")
    print(f"üéØ Scraping –Ω–∞ –º–∞–∫—Å–∏–º—É–º {args.limit} —Å—Ç–∞—Ç–∏–∏...")

    # –°—ä–∑–¥–∞–≤–∞–º–µ scraper
    scraper = CoinDeskScraper(use_database=True)

    # –ü–æ–∫–∞–∑–≤–∞–º–µ –Ω–∞—á–∞–ª–Ω–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    if args.verbose:
        print("\nüìä –ù–∞—á–∞–ª–Ω–∏ database —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:")
        stats = scraper.db.get_database_stats()
        for key, value in stats.items():
            print(f"   {key}: {value}")

    # Scrape-–≤–∞–º–µ —Å—Ç–∞—Ç–∏–∏
    start_time = time.time()
    articles = scraper.scrape_multiple_articles(
        max_articles=args.limit,
        save_to_db=True
    )
    scrape_time = time.time() - start_time

    # –ü–æ–∫–∞–∑–≤–∞–º–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏
    print(f"\nüìà –†–ï–ó–£–õ–¢–ê–¢–ò:")
    print(f"   ‚úÖ –ù–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏: {len(articles)}")
    print(f"   üïí –í—Ä–µ–º–µ: {scrape_time:.1f} —Å–µ–∫—É–Ω–¥–∏")

    # –§–∏–Ω–∞–ª–Ω–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats = scraper.db.get_database_stats()
    print(f"   üìä –û–±—â–æ –≤ –±–∞–∑–∞: {stats['total_articles']} —Å—Ç–∞—Ç–∏–∏")
    print(f"   üìã –ó–∞ –∞–Ω–∞–ª–∏–∑: {stats['unprocessed_articles']} —Å—Ç–∞—Ç–∏–∏")

    if len(articles) > 0:
        print(f"\nüì∞ –ù–∞–π-–Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏:")
        for i, article in enumerate(articles[:3], 1):
            print(f"   {i}. {article['title'][:60]}...")

    return len(articles)


def status_command(args):
    """–ö–æ–º–∞–Ω–¥–∞ –∑–∞ –ø–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ database —Å—Ç–∞—Ç—É—Å"""
    print("=== DATABASE –°–¢–ê–¢–£–° ===")

    db = DatabaseManager()
    stats = db.get_database_stats()

    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:")
    print(f"   üì∞ –û–±—â–æ —Å—Ç–∞—Ç–∏–∏: {stats['total_articles']}")
    print(f"   üìã –ù–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏: {stats['unprocessed_articles']}")
    print(f"   ‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏: {stats['analyzed_articles']}")
    print(f"   üîó Scraped URLs: {stats['total_scraped_urls']}")

    if stats['latest_article']:
        title, date = stats['latest_article']
        print(f"   üìÖ –ù–∞–π-–Ω–æ–≤–∞: {title[:50]}... ({date})")

    # –ü–æ–∫–∞–∑–≤–∞–º–µ –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏
    if args.verbose and stats['unprocessed_articles'] > 0:
        print(f"\nüìã –ù–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏:")
        unprocessed = db.get_unprocessed_articles(limit=5)
        for i, article in enumerate(unprocessed, 1):
            print(f"   {i}. {article['title'][:60]}...")
            print(f"      üìä {article['content_length']} chars | {article['scraped_at']}")


def export_command(args):
    """–ö–æ–º–∞–Ω–¥–∞ –∑–∞ export –Ω–∞ –¥–∞–Ω–Ω–∏"""
    print("=== EXPORT –ù–ê –î–ê–ù–ù–ò ===")

    db = DatabaseManager()

    if args.all:
        count = db.export_articles_to_json(args.output)
        print(f"üì§ –ï–∫—Å–ø–æ—Ä—Ç–∏—Ä–∞–Ω–∏ {count} —Å—Ç–∞—Ç–∏–∏ –≤ {args.output}")
    else:
        count = db.export_articles_to_json(args.output, processed_only=False)
        unprocessed_count = db.get_database_stats()['unprocessed_articles']
        print(f"üì§ –ï–∫—Å–ø–æ—Ä—Ç–∏—Ä–∞–Ω–∏ {unprocessed_count} –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏ –≤ {args.output}")


def cleanup_command(args):
    """–ö–æ–º–∞–Ω–¥–∞ –∑–∞ cleanup –Ω–∞ —Å—Ç–∞—Ä–∏ –¥–∞–Ω–Ω–∏"""
    print("=== CLEANUP –ù–ê –°–¢–ê–†–ò –î–ê–ù–ù–ò ===")

    db = DatabaseManager()

    # –ü–æ–∫–∞–∑–≤–∞–º–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–µ–¥–∏ cleanup
    stats_before = db.get_database_stats()
    print(f"üìä –ü—Ä–µ–¥–∏ cleanup: {stats_before['analyzed_articles']} –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏")

    # –ü—Ä–∞–≤–∏–º cleanup
    if not args.dry_run:
        deleted_count = db.cleanup_old_analyzed_articles(days_to_keep=args.days)
        print(f"üßπ –ò–∑—Ç—Ä–∏—Ç–∏ {deleted_count} –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏ (–ø–æ-—Å—Ç–∞—Ä–∏ –æ—Ç {args.days} –¥–Ω–∏)")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–ª–µ–¥ cleanup
        stats_after = db.get_database_stats()
        print(f"üìä –°–ª–µ–¥ cleanup: {stats_after['total_articles']} –æ–±—â–æ —Å—Ç–∞—Ç–∏–∏")
    else:
        print(f"üîç DRY RUN: –ë–∏ –∏–∑—Ç—Ä–∏–ª –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏ –ø–æ-—Å—Ç–∞—Ä–∏ –æ—Ç {args.days} –¥–Ω–∏")


def analyze_command(args):
    """–ö–æ–º–∞–Ω–¥–∞ –∑–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å sentiment analyzer"""
    print("=== –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –°–™–° SENTIMENT ANALYZER ===")

    db = DatabaseManager()

    # –í–∑–µ–º–∞–º–µ –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏
    unprocessed = db.get_unprocessed_articles(limit=args.limit)

    if not unprocessed:
        print("üìã –ù—è–º–∞ —Å—Ç–∞—Ç–∏–∏ –∑–∞ –∞–Ω–∞–ª–∏–∑")
        return

    print(f"üìã {len(unprocessed)} —Å—Ç–∞—Ç–∏–∏ –∑–∞ –∞–Ω–∞–ª–∏–∑")

    # –ü–æ–¥–≥–æ—Ç–≤—è–º–µ –¥–∞–Ω–Ω–∏ –∑–∞ sentiment analyzer
    articles_for_analysis = []
    for article in unprocessed:
        articles_for_analysis.append({
            'id': article['id'],
            'title': article['title'],
            'content': article['content'],
            'url': article['url']
        })

    # –ï–∫—Å–ø–æ—Ä—Ç–∏—Ä–∞–º–µ –∑–∞ –∞–Ω–∞–ª–∏–∑
    import json
    with open('articles_for_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(articles_for_analysis, f, ensure_ascii=False, indent=2)

    print(f"üì§ –ï–∫—Å–ø–æ—Ä—Ç–∏—Ä–∞–Ω–∏ {len(articles_for_analysis)} —Å—Ç–∞—Ç–∏–∏ –≤ articles_for_analysis.json")
    print(f"üí° –ò–∑–ø–æ–ª–∑–≤–∞–π —Ç–æ–∑–∏ —Ñ–∞–π–ª –≤—ä–≤ —Ç–≤–æ—è sentiment analyzer –ø—Ä–æ–µ–∫—Ç!")
    print(f"üí° –°–ª–µ–¥ –∞–Ω–∞–ª–∏–∑, –º–∞—Ä–∫–∏—Ä–∞–π —Å—Ç–∞—Ç–∏–∏—Ç–µ –∫–∞—Ç–æ processed —Å mark_analyzed –∫–æ–º–∞–Ω–¥–∞")


def mark_analyzed_command(args):
    """–ö–æ–º–∞–Ω–¥–∞ –∑–∞ –º–∞—Ä–∫–∏—Ä–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏–∏ –∫–∞—Ç–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏"""
    print("=== –ú–ê–†–ö–ò–†–ê–ù–ï –ù–ê –°–¢–ê–¢–ò–ò –ö–ê–¢–û –ê–ù–ê–õ–ò–ó–ò–†–ê–ù–ò ===")

    db = DatabaseManager()

    if args.article_id:
        # –ú–∞—Ä–∫–∏—Ä–∞–Ω–µ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ —Å—Ç–∞—Ç–∏—è
        db.mark_article_as_analyzed(args.article_id)
        print(f"‚úÖ –°—Ç–∞—Ç–∏—è {args.article_id} –º–∞—Ä–∫–∏—Ä–∞–Ω–∞ –∫–∞—Ç–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∞")
    elif args.all_processed:
        # –ú–∞—Ä–∫–∏—Ä–∞–Ω–µ –Ω–∞ –≤—Å–∏—á–∫–∏ —Ç–µ–∫—É—â–æ unprocessed –∫–∞—Ç–æ analyzed (–∑–∞ —Ç–µ—Å—Ç–≤–∞–Ω–µ)
        unprocessed = db.get_unprocessed_articles()
        for article in unprocessed:
            db.mark_article_as_analyzed(article['id'])
        print(f"‚úÖ –ú–∞—Ä–∫–∏—Ä–∞–Ω–∏ {len(unprocessed)} —Å—Ç–∞—Ç–∏–∏ –∫–∞—Ç–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏")


def main():
    """–ì–ª–∞–≤–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞ command line –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"""
    parser = argparse.ArgumentParser(
        description="CoinDesk Crypto News Scraper",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä–∏ –∑–∞ –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ:
  python run_scraper.py scrape --limit 10           # Scrape –¥–æ 10 –Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏
  python run_scraper.py status --verbose            # –ü–æ–∫–∞–∂–∏ –ø–æ–¥—Ä–æ–±–µ–Ω —Å—Ç–∞—Ç—É—Å
  python run_scraper.py export --output news.json   # –ï–∫—Å–ø–æ—Ä—Ç –Ω–∞ –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏
  python run_scraper.py cleanup --days 7            # –ò–∑—Ç—Ä–∏–π –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏ >7 –¥–Ω–∏
  python run_scraper.py analyze --limit 5           # –ü–æ–¥–≥–æ—Ç–≤–∏ 5 —Å—Ç–∞—Ç–∏–∏ –∑–∞ –∞–Ω–∞–ª–∏–∑
        """
    )

    subparsers = parser.add_subparsers(dest='command', help='–ù–∞–ª–∏—á–Ω–∏ –∫–æ–º–∞–Ω–¥–∏')

    # Scrape –∫–æ–º–∞–Ω–¥–∞
    scrape_parser = subparsers.add_parser('scrape', help='Scrape –Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏')
    scrape_parser.add_argument('--limit', type=int, default=10, help='–ú–∞–∫—Å–∏–º–∞–ª–µ–Ω –±—Ä–æ–π —Å—Ç–∞—Ç–∏–∏ (default: 10)')
    scrape_parser.add_argument('--verbose', action='store_true', help='–ü–æ–¥—Ä–æ–±–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è')

    # Status –∫–æ–º–∞–Ω–¥–∞
    status_parser = subparsers.add_parser('status', help='–ü–æ–∫–∞–∂–∏ database —Å—Ç–∞—Ç—É—Å')
    status_parser.add_argument('--verbose', action='store_true', help='–ü–æ–∫–∞–∂–∏ –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏')

    # Export –∫–æ–º–∞–Ω–¥–∞
    export_parser = subparsers.add_parser('export', help='–ï–∫—Å–ø–æ—Ä—Ç –Ω–∞ –¥–∞–Ω–Ω–∏')
    export_parser.add_argument('--output', default='articles.json', help='Output —Ñ–∞–π–ª (default: articles.json)')
    export_parser.add_argument('--all', action='store_true', help='–ï–∫—Å–ø–æ—Ä—Ç –Ω–∞ –≤—Å–∏—á–∫–∏ —Å—Ç–∞—Ç–∏–∏')

    # Cleanup –∫–æ–º–∞–Ω–¥–∞
    cleanup_parser = subparsers.add_parser('cleanup', help='Cleanup –Ω–∞ —Å—Ç–∞—Ä–∏ –¥–∞–Ω–Ω–∏')
    cleanup_parser.add_argument('--days', type=int, default=7, help='–î–Ω–∏ –∑–∞ –ø–∞–∑–µ–Ω–µ (default: 7)')
    cleanup_parser.add_argument('--dry-run', action='store_true', help='–°–∞–º–æ –ø–æ–∫–∞–∑–≤–∞ –∫–∞–∫–≤–æ —â–µ –Ω–∞–ø—Ä–∞–≤–∏')

    # Analyze –∫–æ–º–∞–Ω–¥–∞
    analyze_parser = subparsers.add_parser('analyze', help='–ü–æ–¥–≥–æ—Ç–≤–∏ —Å—Ç–∞—Ç–∏–∏ –∑–∞ sentiment –∞–Ω–∞–ª–∏–∑')
    analyze_parser.add_argument('--limit', type=int, default=5, help='–ë—Ä–æ–π —Å—Ç–∞—Ç–∏–∏ –∑–∞ –∞–Ω–∞–ª–∏–∑ (default: 5)')

    # Mark analyzed –∫–æ–º–∞–Ω–¥–∞
    mark_parser = subparsers.add_parser('mark_analyzed', help='–ú–∞—Ä–∫–∏—Ä–∞–π —Å—Ç–∞—Ç–∏–∏ –∫–∞—Ç–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏')
    mark_parser.add_argument('--article-id', type=int, help='ID –Ω–∞ —Å—Ç–∞—Ç–∏—è –∑–∞ –º–∞—Ä–∫–∏—Ä–∞–Ω–µ')
    mark_parser.add_argument('--all-processed', action='store_true', help='–ú–∞—Ä–∫–∏—Ä–∞–π –≤—Å–∏—á–∫–∏ –∫–∞—Ç–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏')

    # Parse –∞—Ä–≥—É–º–µ–Ω—Ç–∏—Ç–µ
    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    try:
        # –ò–∑–ø—ä–ª–Ω—è–≤–∞–Ω–µ –Ω–∞ –∫–æ–º–∞–Ω–¥–∏—Ç–µ
        if args.command == 'scrape':
            article_count = scrape_command(args)
            if article_count == 0:
                print("\n‚ÑπÔ∏è –ù—è–º–∞ –Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏ –∑–∞ scraping. –û–ø–∏—Ç–∞–π –ø–∞–∫ –ø–æ-–∫—ä—Å–Ω–æ.")
        elif args.command == 'status':
            status_command(args)
        elif args.command == 'export':
            export_command(args)
        elif args.command == 'cleanup':
            cleanup_command(args)
        elif args.command == 'analyze':
            analyze_command(args)
        elif args.command == 'mark_analyzed':
            mark_analyzed_command(args)

        print(f"\n‚úÖ –ö–æ–º–∞–Ω–¥–∞ '{args.command}' –∑–∞–≤—ä—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –ü—Ä–µ–∫—Ä–∞—Ç–µ–Ω–æ –æ—Ç –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –ì—Ä–µ—à–∫–∞: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
