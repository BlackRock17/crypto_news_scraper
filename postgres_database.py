import psycopg2
import psycopg2.extras
import json
from datetime import datetime
import os


class PostgreSQLDatabaseManager:
        def __init__(self):
            print("üêò –°–≤—ä—Ä–∑–≤–∞–Ω–µ —Å PostgreSQL...")

            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞ –±–∞–∑–∞—Ç–∞
            self.db_config = {
                'host': 'localhost',
                'port': 5432,
                'database': 'crypto_news',
                'user': 'crypto_user',
                'password': 'password'
            }

            # –¢–µ—Å—Ç–≤–∞–º–µ –≤—Ä—ä–∑–∫–∞—Ç–∞
            self._test_connection()
            self.init_database()
            print("‚úÖ PostgreSQL –≥–æ—Ç–æ–≤!")

        def _test_connection(self):
            """–ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ –º–æ–∂–µ –¥–∞ —Å–µ —Å–≤—ä—Ä–∂–µ —Å PostgreSQL"""
            try:
                conn = psycopg2.connect(**self.db_config)
                conn.close()
                print("‚úÖ –í—Ä—ä–∑–∫–∞ —Å PostgreSQL —É—Å–ø–µ—à–Ω–∞")
            except psycopg2.Error as e:
                print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å–≤—ä—Ä–∑–≤–∞–Ω–µ: {e}")
                raise

        def get_connection(self):
            """–í—Ä—ä—â–∞ –Ω–æ–≤–∞ –≤—Ä—ä–∑–∫–∞ –∫—ä–º –±–∞–∑–∞—Ç–∞"""
            return psycopg2.connect(**self.db_config)

        def init_database(self):
            """–°—ä–∑–¥–∞–≤–∞ —Ç–∞–±–ª–∏—Ü–∏—Ç–µ –∑–∞ –±–∞–∑–∞ –¥–∞–Ω–Ω–∏ –ê (—Å—Ç–∞—Ç–∏–∏ –∑–∞ scraping)"""
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    # –¢–∞–±–ª–∏—Ü–∞ –∑–∞ —Å—Ç–∞—Ç–∏–∏
                    cursor.execute('''
                        CREATE TABLE IF NOT EXISTS articles (
                            id SERIAL PRIMARY KEY,
                            url TEXT UNIQUE NOT NULL,
                            title TEXT NOT NULL,
                            content TEXT NOT NULL,
                            author TEXT,
                            published_date TEXT,
                            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            content_length INTEGER,
                            is_analyzed BOOLEAN DEFAULT FALSE
                        )
                    ''')

                    # –¢–∞–±–ª–∏—Ü–∞ –∑–∞ scraped URLs (–∑–∞ –¥–∞ –Ω–µ –ø–æ–≤—Ç–∞—Ä—è–º–µ)
                    cursor.execute('''
                        CREATE TABLE IF NOT EXISTS scraped_urls (
                            id SERIAL PRIMARY KEY,
                            url TEXT UNIQUE NOT NULL,
                            first_scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            last_seen_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            scrape_count INTEGER DEFAULT 1
                        )
                    ''')

                    # –ò–Ω–¥–µ–∫—Å–∏ –∑–∞ –ø–æ-–±—ä—Ä–∑–∏ –∑–∞—è–≤–∫–∏
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_url ON articles(url)')
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_is_analyzed ON articles(is_analyzed)')
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_scraped_urls_url ON scraped_urls(url)')

                    print("‚úÖ –¢–∞–±–ª–∏—Ü–∏ –∑–∞ –±–∞–∑–∞ –¥–∞–Ω–Ω–∏ –ê —Å—ä–∑–¥–∞–¥–µ–Ω–∏")
                    conn.commit()

        def save_article(self, article_data):
            try:
                with self.get_connection() as conn:
                    with conn.cursor() as cursor:
                        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞–ª–∏ —Å—Ç–∞—Ç–∏—è—Ç–∞ –≤–µ—á–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞
                        cursor.execute("SELECT 1 FROM articles WHERE url = %s", (article_data['url'],))
                        if cursor.fetchone():
                            print(f"‚ö†Ô∏è –°—Ç–∞—Ç–∏—è—Ç–∞ –≤–µ—á–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞: {article_data['title'][:50]}...")
                            return False

                        # –ó–∞–ø–∞–∑–≤–∞–º–µ —Å—Ç–∞—Ç–∏—è—Ç–∞
                        cursor.execute('''
                            INSERT INTO articles 
                            (url, title, content, author, published_date, content_length)
                            VALUES (%s, %s, %s, %s, %s, %s)
                        ''', (
                            article_data['url'],
                            article_data['title'],
                            article_data['content'],
                            article_data['author'],
                            str(article_data['date']),
                            article_data['content_length']
                        ))

                        conn.commit()
                        print(f"‚úÖ –ó–∞–ø–∞–∑–µ–Ω–∞ —Å—Ç–∞—Ç–∏—è: {article_data['title'][:50]}...")

                        # –î–û–ë–ê–í–ò –¢–û–ó–ò –†–ï–î:
                        self.record_scraped_url(article_data['url'])

                        return True

            except psycopg2.Error as e:
                print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∞–∑–≤–∞–Ω–µ: {e}")
                return False


            except psycopg2.Error as e:
                print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∞–∑–≤–∞–Ω–µ: {e}")
                return False

        def save_multiple_articles(self, articles):
            """–ó–∞–ø–∞–∑–≤–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–∏–∏ –Ω–∞–≤–µ–¥–Ω—ä–∂ (–ø–æ-–±—ä—Ä–∑–æ)"""
            print(f"üíæ –ó–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ {len(articles)} —Å—Ç–∞—Ç–∏–∏...")

            saved_count = 0
            duplicate_count = 0

            try:
                with self.get_connection() as conn:
                    with conn.cursor() as cursor:

                        for article in articles:
                            # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –∑–∞ –¥—É–±–ª–∏—Ä–∞—â–∏ —Å–µ —Å—Ç–∞—Ç–∏–∏
                            cursor.execute("SELECT 1 FROM articles WHERE url = %s", (article['url'],))
                            if cursor.fetchone():
                                duplicate_count += 1
                                continue

                            # –ó–∞–ø–∞–∑–≤–∞–º–µ —Å—Ç–∞—Ç–∏—è—Ç–∞
                            cursor.execute('''
                                INSERT INTO articles 
                                (url, title, content, author, published_date, content_length)
                                VALUES (%s, %s, %s, %s, %s, %s)
                            ''', (
                                article['url'],
                                article['title'],
                                article['content'],
                                article['author'],
                                str(article['date']),
                                article['content_length']
                            ))

                            saved_count += 1

                        conn.commit()

            except psycopg2.Error as e:
                print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∞–∑–≤–∞–Ω–µ: {e}")

            print(f"üìä –†–µ–∑—É–ª—Ç–∞—Ç: {saved_count} –Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏, {duplicate_count} –¥—É–±–ª–∏—Ä–∞—â–∏ —Å–µ")
            return saved_count, duplicate_count

        def is_url_scraped_before(self, url):
            """–ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ URL-–∞ –µ –±–∏–ª —Å–∫—Ä–∞–ø–≤–∞–Ω –ø—Ä–µ–¥–∏"""
            try:
                with self.get_connection() as conn:
                    with conn.cursor() as cursor:
                        cursor.execute("SELECT 1 FROM scraped_urls WHERE url = %s", (url,))
                        return cursor.fetchone() is not None
            except psycopg2.Error:
                return False

        def record_scraped_url(self, url):
            """–ó–∞–ø–∏—Å–≤–∞ URL –≤ –∏—Å—Ç–æ—Ä–∏—è—Ç–∞ (–∑–∞ –¥–∞ –Ω–µ –≥–æ —Å–∫—Ä–∞–ø–≤–∞–º–µ –ø–∞–∫)"""
            try:
                with self.get_connection() as conn:
                    with conn.cursor() as cursor:
                        # PostgreSQL UPSERT —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
                        cursor.execute('''
                            INSERT INTO scraped_urls (url) 
                            VALUES (%s)
                            ON CONFLICT (url) 
                            DO UPDATE SET 
                                last_seen_at = CURRENT_TIMESTAMP,
                                scrape_count = scraped_urls.scrape_count + 1
                        ''', (url,))

                        conn.commit()
                        return True
            except psycopg2.Error as e:
                print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–≤–∞–Ω–µ –Ω–∞ URL: {e}")
                return False

        def get_database_stats(self):
            """–ü–æ–∫–∞–∑–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞ –±–∞–∑–∞—Ç–∞ –¥–∞–Ω–Ω–∏"""
            try:
                with self.get_connection() as conn:
                    with conn.cursor() as cursor:
                        # –û–±—â –±—Ä–æ–π —Å—Ç–∞—Ç–∏–∏
                        cursor.execute("SELECT COUNT(*) FROM articles")
                        total_articles = cursor.fetchone()[0]

                        # –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏
                        cursor.execute("SELECT COUNT(*) FROM articles WHERE is_analyzed = TRUE")
                        analyzed_articles = cursor.fetchone()[0]

                        # –ù–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏
                        cursor.execute("SELECT COUNT(*) FROM articles WHERE is_analyzed = FALSE")
                        unanalyzed_articles = cursor.fetchone()[0]

                        return {
                            'total_articles': total_articles,
                            'analyzed_articles': analyzed_articles,
                            'unprocessed_articles': unanalyzed_articles  # ‚Üê –¢–û–ó–ò –†–ï–î
                        }
            except psycopg2.Error as e:
                print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
                return {}
