import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
from urllib.parse import urljoin
import re

from config import (
    COINDESK_MAIN_PAGE,
    REQUEST_HEADERS,
    NEWS_URL_PATTERNS,
    SCRAPING_CONFIG,
    HTML_SELECTORS,
    is_valid_article_url,
    get_full_url
)
from database import DatabaseManager


class CoinDeskScraper:
    def __init__(self, use_database=True):
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ CoinDesk Scraper...")
        self.session = requests.Session()

        # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ –ø–æ-–ø—Ä–æ—Å—Ç–∏ headers –∫–∞—Ç–æ –≤ debug —Å–∫—Ä–∏–ø—Ç–∞
        simple_headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.session.headers.update(simple_headers)
        self.scraped_urls = set()  # –ó–∞ –¥–∞ –∏–∑–±—è–≥–≤–∞–º–µ –¥—É–±–ª–∏—Ä–∞–Ω–µ

        # Database –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
        self.use_database = use_database
        if use_database:
            self.db = DatabaseManager()
        else:
            self.db = None

        print("‚úÖ Scraper –≥–æ—Ç–æ–≤!")

    def get_article_links(self):
        """–ù–∞–º–∏—Ä–∞ –≤—Å–∏—á–∫–∏ –ª–∏–Ω–∫–æ–≤–µ –∫—ä–º —Å—Ç–∞—Ç–∏–∏ –æ—Ç –≥–ª–∞–≤–Ω–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
        print("üîç –¢—ä—Ä—Å–µ–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏–∏ –≤ –≥–ª–∞–≤–Ω–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞...")

        try:
            response = self.session.get(
                COINDESK_MAIN_PAGE,
                timeout=SCRAPING_CONFIG['request_timeout']
            )
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # –ù–∞–º–∏—Ä–∞–º–µ –≤—Å–∏—á–∫–∏ –ª–∏–Ω–∫–æ–≤–µ
            all_links = soup.find_all('a', href=True)

            # –§–∏–ª—Ç—Ä–∏—Ä–∞–º–µ –≤–∞–ª–∏–¥–Ω–∏—Ç–µ —Å—Ç–∞—Ç–∏–∏
            article_links = []
            for link in all_links:
                href = link['href']

                # –ü—Ä–∞–≤–∏–º –ø—ä–ª–µ–Ω URL
                full_url = get_full_url(href)

                # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞–ª–∏ –µ –≤–∞–ª–∏–¥–µ–Ω article URL
                if is_valid_article_url(href):
                    # –í–∑–µ–º–∞–º–µ –∑–∞–≥–ª–∞–≤–∏–µ—Ç–æ –Ω–∞ –ª–∏–Ω–∫–∞
                    title = link.text.strip()
                    if title and len(title) > 10:  # –°–∞–º–æ —Å—ä—Å —Å–º–∏—Å–ª–µ–Ω–æ –∑–∞–≥–ª–∞–≤–∏–µ
                        article_links.append({
                            'url': full_url,
                            'title': title,
                            'href': href
                        })

            # –ü—Ä–µ–º–∞—Ö–≤–∞–º–µ –¥—É–±–ª–∏—Ä–∞–Ω–∏—Ç–µ URLs
            unique_articles = []
            seen_urls = set()
            for article in article_links:
                if article['url'] not in seen_urls:
                    unique_articles.append(article)
                    seen_urls.add(article['url'])

            print(f"üì∞ –ù–∞–º–µ—Ä–µ–Ω–∏ {len(unique_articles)} —É–Ω–∏–∫–∞–ª–Ω–∏ —Å—Ç–∞—Ç–∏–∏")
            return unique_articles

        except Exception as e:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–∏—á–∞–Ω–µ—Ç–æ –Ω–∞ –ª–∏–Ω–∫–æ–≤–µ: {str(e)}")
            return []

    def scrape_single_article(self, article_url):
        """–ò–∑–≤–ª–∏—á–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–∞ –µ–¥–Ω–∞ —Å—Ç–∞—Ç–∏—è"""
        print(f"üìÑ Scraping —Å—Ç–∞—Ç–∏—è: {article_url}")

        try:
            # –ú–∞–ª–∫–∞ –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞—è–≤–∫–∏—Ç–µ
            time.sleep(SCRAPING_CONFIG['delay_between_requests'])

            response = self.session.get(
                article_url,
                timeout=SCRAPING_CONFIG['request_timeout']
            )
            response.raise_for_status()

            # –ü—Ä–∞–≤–∏–ª–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–∞–Ω–µ –∫–∞—Ç–æ –≤ debug —Å–∫—Ä–∏–ø—Ç–∞
            content_text = response.content.decode('utf-8', errors='ignore')
            soup = BeautifulSoup(content_text, 'html.parser')

            # –ò–∑–≤–ª–∏—á–∞–º–µ –∑–∞–≥–ª–∞–≤–∏–µ—Ç–æ
            title = self._extract_title(soup)

            # –ò–∑–≤–ª–∏—á–∞–º–µ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ
            content = self._extract_content(soup)

            # –ò–∑–≤–ª–∏—á–∞–º–µ –¥–∞—Ç–∞—Ç–∞
            date = self._extract_date(soup)

            # –ò–∑–≤–ª–∏—á–∞–º–µ –∞–≤—Ç–æ—Ä–∞ (–∞–∫–æ –∏–º–∞)
            author = self._extract_author(soup)

            # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞–ª–∏ —Å—Ç–∞—Ç–∏—è—Ç–∞ –µ –¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ –¥—ä–ª–≥–∞
            if len(content) < SCRAPING_CONFIG['min_article_length']:
                print(f"‚ö†Ô∏è  –°—Ç–∞—Ç–∏—è—Ç–∞ –µ —Ç–≤—ä—Ä–¥–µ –∫—Ä–∞—Ç–∫–∞ ({len(content)} chars)")
                return None

            article_data = {
                'url': article_url,
                'title': title,
                'content': content,
                'date': date,
                'author': author,
                'scraped_at': datetime.now(),
                'content_length': len(content)
            }

            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—è: {title[:50]}...")
            return article_data

        except Exception as e:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ scraping –Ω–∞ {article_url}: {str(e)}")
            return None

    def _extract_title(self, soup):
        """–ò–∑–≤–ª–∏—á–∞ –∑–∞–≥–ª–∞–≤–∏–µ—Ç–æ –Ω–∞ —Å—Ç–∞—Ç–∏—è—Ç–∞"""
        for selector in HTML_SELECTORS['article_title']:
            title_element = soup.select_one(selector)
            if title_element:
                return title_element.get_text().strip()

        # Fallback - —Ç—ä—Ä—Å–∏–º –≤ <title> tag
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
            # –ü–æ—á–∏—Å—Ç–≤–∞–º–µ –æ—Ç " | CoinDesk" –≤ –∫—Ä–∞—è
            title = re.sub(r'\s*\|\s*CoinDesk.*$', '', title)
            return title

        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –∑–∞–≥–ª–∞–≤–∏–µ"

    def _extract_content(self, soup):
        """–ò–∑–≤–ª–∏—á–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–∞ —Å—Ç–∞—Ç–∏—è—Ç–∞"""
        # –ü—ä—Ä–≤–æ –æ–ø–∏—Ç–≤–∞–º–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏—Ç–µ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏
        for selector in HTML_SELECTORS['article_content'][1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–º–µ 'p' –∑–∞ —Å–µ–≥–∞
            content_element = soup.select_one(selector)
            if content_element:
                # –í–∑–µ–º–∞–º–µ –≤—Å–∏—á–∫–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏
                paragraphs = content_element.find_all('p')
                content = '\n\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
                if content:
                    return content

        # Fallback - –∏–∑–ø–æ–ª–∑–≤–∞–º–µ –≤—Å–∏—á–∫–∏ <p> tags (—Ç–æ–≤–∞ —Ä–∞–±–æ—Ç–∏ –∑–∞ CoinDesk!)
        all_paragraphs = soup.find_all('p')
        if all_paragraphs:
            # –§–∏–ª—Ç—Ä–∏—Ä–∞–º–µ —Å–∞–º–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏—Ç–µ —Å—ä—Å —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ
            meaningful_paragraphs = []
            for p in all_paragraphs:
                text = p.get_text().strip()
                # –í–∑–∏–º–∞–º–µ —Å–∞–º–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏ —Å –¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ —Ç–µ–∫—Å—Ç
                if len(text) > 30 and not text.startswith('Sign up') and not text.startswith('Get the'):
                    meaningful_paragraphs.append(text)

            content = '\n\n'.join(meaningful_paragraphs)
            return content

        return "–°—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–µ –º–æ–∂–µ –¥–∞ –±—ä–¥–µ –∏–∑–≤–ª–µ—á–µ–Ω–æ"

    def _extract_date(self, soup):
        """–ò–∑–≤–ª–∏—á–∞ –¥–∞—Ç–∞—Ç–∞ –Ω–∞ —Å—Ç–∞—Ç–∏—è—Ç–∞"""
        for selector in HTML_SELECTORS['article_date']:
            date_element = soup.select_one(selector)
            if date_element:
                # –¢—ä—Ä—Å–∏–º datetime –∞—Ç—Ä–∏–±—É—Ç
                datetime_str = date_element.get('datetime')
                if datetime_str:
                    try:
                        return datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
                    except:
                        pass

                # –û–ø–∏—Ç–≤–∞–º–µ –¥–∞ –ø–∞—Ä—Å–∏—Ä–∞–º–µ —Ç–µ–∫—Å—Ç–∞
                date_text = date_element.get_text().strip()
                if date_text:
                    return date_text

        return datetime.now().strftime('%Y-%m-%d')

    def _extract_author(self, soup):
        """–ò–∑–≤–ª–∏—á–∞ –∞–≤—Ç–æ—Ä–∞ –Ω–∞ —Å—Ç–∞—Ç–∏—è—Ç–∞"""
        for selector in HTML_SELECTORS['article_author']:
            author_element = soup.select_one(selector)
            if author_element:
                return author_element.get_text().strip()

        return "–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω –∞–≤—Ç–æ—Ä"

    def scrape_multiple_articles(self, max_articles=None, save_to_db=True):
        """Scrape-–≤–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–∏–∏ –∏ –≥–∏ –∑–∞–ø–∞–∑–≤–∞ –≤ –±–∞–∑–∞—Ç–∞ –¥–∞–Ω–Ω–∏"""
        if max_articles is None:
            max_articles = SCRAPING_CONFIG['max_articles_per_session']

        print(f"üéØ –ó–∞–ø–æ—á–≤–∞–º scraping –Ω–∞ –º–∞–∫—Å–∏–º—É–º {max_articles} —Å—Ç–∞—Ç–∏–∏...")

        # –ü–æ–ª—É—á–∞–≤–∞–º–µ –ª–∏–Ω–∫–æ–≤–µ—Ç–µ
        article_links = self.get_article_links()

        if not article_links:
            print("‚ùå –ù–µ —Å–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —Å—Ç–∞—Ç–∏–∏ –∑–∞ scraping")
            return []

        # –ê–∫–æ –∏–∑–ø–æ–ª–∑–≤–∞–º–µ database, —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–º–µ –≤–µ—á–µ scraped URLs
        if self.db and save_to_db:
            print("üîç –ü—Ä–æ–≤–µ—Ä—è–≤–∞–Ω–µ –∑–∞ –¥—É–±–ª–∏—Ä–∞—â–∏ —Å–µ URLs...")
            new_article_links = []
            skipped_count = 0

            for link_info in article_links:
                url = link_info['url']
                if not self.db.is_url_scraped_before(url):
                    new_article_links.append(link_info)
                else:
                    skipped_count += 1
                    # Update –ø–æ—Å–ª–µ–¥–Ω–æ –≤–∏–∂–¥–∞–Ω–µ
                    self.db.record_scraped_url(url)

            print(f"üìä {len(new_article_links)} –Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏, {skipped_count} –≤–µ—á–µ scraped")
            article_links = new_article_links

        # –û–≥—Ä–∞–Ω–∏—á–∞–≤–∞–º–µ –±—Ä–æ—è
        article_links = article_links[:max_articles]

        if not article_links:
            print("‚ÑπÔ∏è –í—Å–∏—á–∫–∏ —Å—Ç–∞—Ç–∏–∏ —Å–∞ –≤–µ—á–µ scraped")
            return []

        # Scrape-–≤–∞–º–µ –≤—Å—è–∫–∞ —Å—Ç–∞—Ç–∏—è
        scraped_articles = []
        for i, link_info in enumerate(article_links, 1):
            url = link_info['url']
            print(f"\n[{i}/{len(article_links)}] {link_info['title'][:60]}...")

            article_data = self.scrape_single_article(url)
            if article_data:
                scraped_articles.append(article_data)

                # –ó–∞–ø–∞–∑–≤–∞–º–µ –≤ –±–∞–∑–∞—Ç–∞ –¥–∞–Ω–Ω–∏ –∞–∫–æ –µ –∑–∞—è–≤–µ–Ω–æ
                if self.db and save_to_db:
                    self.db.save_article(article_data)

            # –ü—Ä–æ–≥—Ä–µ—Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            if i % 5 == 0:
                print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å: {i}/{len(article_links)} —Å—Ç–∞—Ç–∏–∏ –æ–±—Ä–∞–±–æ—Ç–µ–Ω–∏")

        print(f"\nüéâ Scraping –∑–∞–≤—ä—Ä—à–µ–Ω! –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏ {len(scraped_articles)} —Å—Ç–∞—Ç–∏–∏")

        # Database —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if self.db and save_to_db:
            stats = self.db.get_database_stats()
            print(
                f"üìä Database —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {stats['total_articles']} –æ–±—â–æ —Å—Ç–∞—Ç–∏–∏, {stats['unprocessed_articles']} –∑–∞ –∞–Ω–∞–ª–∏–∑")

        return scraped_articles


# –¢–µ—Å—Ç–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏—è
def test_single_article():
    """–¢–µ—Å—Ç–≤–∞ scraping –Ω–∞ –µ–¥–Ω–∞ —Å—Ç–∞—Ç–∏—è"""
    scraper = CoinDeskScraper()

    # –¢–µ—Å—Ç–≤–∞–º–µ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ —Å—Ç–∞—Ç–∏—è
    test_url = "https://www.coindesk.com/markets/2025/06/09/bitwise-proshares-file-for-etfs-tracking-soaring-circle-shares"

    print(f"\nüß™ –¢–µ—Å—Ç–≤–∞–Ω–µ –Ω–∞ scraping –Ω–∞ –µ–¥–Ω–∞ —Å—Ç–∞—Ç–∏—è...")
    article = scraper.scrape_single_article(test_url)

    if article:
        print(f"\n‚úÖ –£–°–ü–ï–•! –ò–∑–≤–ª–µ—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—è:")
        print(f"   üìÑ –ó–∞–≥–ª–∞–≤–∏–µ: {article['title']}")
        print(f"   üìÖ –î–∞—Ç–∞: {article['date']}")
        print(f"   üë§ –ê–≤—Ç–æ—Ä: {article['author']}")
        print(f"   üìä –î—ä–ª–∂–∏–Ω–∞: {article['content_length']} —Å–∏–º–≤–æ–ª–∞")
        print(f"   üìù –ü—ä—Ä–≤–∏ 200 —Å–∏–º–≤–æ–ª–∞: {article['content'][:200]}...")
        return True
    else:
        print("‚ùå –ù–µ—É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏—è—Ç–∞")
        return False


if __name__ == "__main__":
    print("=== COINDESK SCRAPER TEST ===")

    # –¢–µ—Å—Ç–≤–∞–º–µ –æ—Å–Ω–æ–≤–Ω–∞—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç
    test_single_article()
