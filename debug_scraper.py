import requests
from bs4 import BeautifulSoup
from config import REQUEST_HEADERS


def test_simple_request():
    """–ü—Ä–æ—Å—Ç —Ç–µ—Å—Ç –∑–∞ –¥–∞ –≤–∏–¥–∏–º –∫–∞–∫–≤–æ –ø–æ–ª—É—á–∞–≤–∞–º–µ"""
    url = "https://www.coindesk.com/"  # –ì–ª–∞–≤–Ω–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞

    print(f"üîç –¢–µ—Å—Ç–≤–∞–Ω–µ –Ω–∞ –æ—Å–Ω–æ–≤–Ω–∞ –∑–∞—è–≤–∫–∞ –∫—ä–º: {url}")

    try:
        # –û–ø–∏—Ç–≤–∞–º–µ —Å —Ä–∞–∑–ª–∏—á–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        session = requests.Session()

        # –ü–æ-–ø—Ä–æ—Å—Ç–∏ headers
        simple_headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        session.headers.update(simple_headers)

        response = session.get(url, timeout=15)
        print(f"üìä Status –∫–æ–¥: {response.status_code}")
        print(f"üìÑ Content-Type: {response.headers.get('content-type', 'Unknown')}")
        print(f"üì¶ Content-Length: {len(response.content)} bytes")
        print(f"üóúÔ∏è Content-Encoding: {response.headers.get('content-encoding', 'None')}")

        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞–ª–∏ –ø–æ–ª—É—á–∞–≤–∞–º–µ HTML
        if 'text/html' in response.headers.get('content-type', ''):
            # –§–æ—Ä—Å–∏—Ä–∞–º–µ –ø—Ä–∞–≤–∏–ª–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–∞–Ω–µ
            content = response.content

            # –û–ø–∏—Ç–≤–∞–º–µ —Ä–∞–∑–ª–∏—á–Ω–∏ –Ω–∞—á–∏–Ω–∏ –∑–∞ –¥–µ–∫–æ–¥–∏—Ä–∞–Ω–µ
            try:
                # 1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
                text_auto = response.text
                print(f"üìù Auto decode: {len(text_auto)} chars")

                # 2. UTF-8 –¥–∏—Ä–µ–∫—Ç–Ω–æ
                text_utf8 = content.decode('utf-8', errors='ignore')
                print(f"üìù UTF-8 decode: {len(text_utf8)} chars")

                # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ UTF-8 –≤–µ—Ä—Å–∏—è—Ç–∞
                soup = BeautifulSoup(text_utf8, 'html.parser')

                # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –æ—Å–Ω–æ–≤–Ω–∏—Ç–µ –µ–ª–µ–º–µ–Ω—Ç–∏
                title = soup.find('title')
                print(f"üìÑ Title tag: {title.get_text() if title else 'None'}")

                # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –ª–∏–Ω–∫–æ–≤–µ
                links = soup.find_all('a', href=True)
                print(f"üîó –ù–∞–º–µ—Ä–µ–Ω–∏ lin–∫–æ–≤–µ: {len(links)}")

                # –ü–æ–∫–∞–∑–≤–∞–º–µ –ø—ä—Ä–≤–∏—Ç–µ –Ω—è–∫–æ–ª–∫–æ –ª–∏–Ω–∫–∞
                for i, link in enumerate(links[:5], 1):
                    href = link.get('href', '')
                    text = link.get_text().strip()[:50]
                    print(f"  {i}. {text} ‚Üí {href}")

                return soup

            except Exception as decode_error:
                print(f"‚ùå –î–µ–∫–æ–¥–∏—Ä–∞–Ω–µ –≥—Ä–µ—à–∫–∞: {decode_error}")
                return None
        else:
            print("‚ùå –ù–µ –ø–æ–ª—É—á–∏—Ö–º–µ HTML —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ")
            return None

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—è–≤–∫–∞—Ç–∞: {str(e)}")
        return None


def debug_article_structure(url):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä–∞ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–∞—Ç–∞ –Ω–∞ —Å—Ç–∞—Ç–∏—è –∑–∞ –¥–∞ –Ω–∞–º–µ—Ä–∏–º –ø—Ä–∞–≤–∏–ª–Ω–∏—Ç–µ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏"""
    print(f"\nüîç Debugging HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–∞: {url}")

    try:
        session = requests.Session()

        # –ü–æ-–ø—Ä–æ—Å—Ç–∏ headers
        simple_headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        session.headers.update(simple_headers)

        response = session.get(url, timeout=15)
        response.raise_for_status()

        print(f"üìä Status –∫–æ–¥: {response.status_code}")
        print(f"üìÑ Content-Type: {response.headers.get('content-type')}")

        # –§–æ—Ä—Å–∏—Ä–∞–º–µ UTF-8 –¥–µ–∫–æ–¥–∏—Ä–∞–Ω–µ
        content_text = response.content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(content_text, 'html.parser')

        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –∑–∞–≥–ª–∞–≤–∏–µ—Ç–æ
        print("\n=== –ó–ê–ì–õ–ê–í–ò–ï ===")
        possible_titles = [
            soup.find('h1'),
            soup.find('title'),
            soup.select_one('[data-module="ArticleHeader"] h1'),
            soup.select_one('.headline'),
        ]

        for i, title in enumerate(possible_titles, 1):
            if title:
                print(f"  {i}. {title.name}: {title.get_text().strip()[:100]}")
            else:
                print(f"  {i}. None")

        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ
        print("\n=== –°–™–î–™–†–ñ–ê–ù–ò–ï ===")

        # –¢—ä—Ä—Å–∏–º —Ä–∞–∑–ª–∏—á–Ω–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –∑–∞ —Å—Ç–∞—Ç–∏–∏
        content_selectors = [
            '[data-module="ArticleBody"]',
            '.article-content',
            '.post-content',
            'article',
            '.entry-content',
            'main article',
            '.content'
        ]

        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                content = elements[0].get_text().strip()
                print(f"‚úÖ {selector}: {len(content)} chars - {content[:150]}...")

                # –ü–æ–∫–∞–∑–≤–∞–º–µ –∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏—Ç–µ
                paragraphs = elements[0].find_all('p')
                print(f"   üìù –ü–∞—Ä–∞–≥—Ä–∞—Ñ–∏: {len(paragraphs)}")
                if paragraphs:
                    print(f"   üìù –ü—ä—Ä–≤–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ: {paragraphs[0].get_text().strip()[:100]}...")
            else:
                print(f"‚ùå {selector}: –ù–µ –Ω–∞–º–µ—Ä–µ–Ω")

        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –≤—Å–∏—á–∫–∏ <p> tags
        all_paragraphs = soup.find_all('p')
        print(f"\nüì∞ –û–±—â–æ <p> tags: {len(all_paragraphs)}")

        # –ü–æ–∫–∞–∑–≤–∞–º–µ –ø—ä—Ä–≤–∏—Ç–µ 5 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
        print("\n=== –ü–™–†–í–ò 5 –ü–ê–†–ê–ì–†–ê–§–ê ===")
        for i, p in enumerate(all_paragraphs[:5], 1):
            text = p.get_text().strip()
            if text:
                print(f"  {i}. ({len(text)} chars) {text[:100]}...")
            else:
                print(f"  {i}. (–ø—Ä–∞–∑–µ–Ω)")

        # –ó–∞–ø–∞–∑–≤–∞–º–µ HTML –∑–∞ debugging (–ø—ä—Ä–≤–∏—Ç–µ 2000 —Å–∏–º–≤–æ–ª–∞)
        print(f"\n=== HTML PREVIEW (–ø—ä—Ä–≤–∏ 1000 chars) ===")
        html_preview = content_text[:1000]
        print(html_preview)

        return True

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ debugging: {str(e)}")
        return False


if __name__ == "__main__":
    print("=== DEBUGGING COINDESK SCRAPER ===")

    # –ü—ä—Ä–≤–æ —Ç–µ—Å—Ç–≤–∞–º–µ –≥–ª–∞–≤–Ω–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞
    print("1. –¢–µ—Å—Ç–≤–∞–Ω–µ –Ω–∞ –≥–ª–∞–≤–Ω–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞...")
    soup = test_simple_request()

    if soup:
        print("\n2. –¢–µ—Å—Ç–≤–∞–Ω–µ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ —Å—Ç–∞—Ç–∏—è...")
        # –¢–µ—Å—Ç–≤–∞–º–µ —Å –∞–∫—Ç—É–∞–ª–Ω–∞ —Å—Ç–∞—Ç–∏—è
        test_url = "https://www.coindesk.com/markets/2025/06/09/bitwise-proshares-file-for-etfs-tracking-soaring-circle-shares"
        debug_article_structure(test_url)
    else:
        print("‚ùå –ù–µ –º–æ–∂–µ–º –¥–∞ –¥–æ—Å—Ç–∏–≥–Ω–µ–º CoinDesk. –í—ä–∑–º–æ–∂–Ω–æ –µ —Å–∞–π—Ç—ä—Ç –¥–∞ –±–ª–æ–∫–∏—Ä–∞ scraping.")
