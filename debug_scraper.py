#!/usr/bin/env python3
"""
Debug –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∑–∞ –∞–Ω–∞–ª–∏–∑ –Ω–∞ CoinDesk Scraper –ø–æ–≤–µ–¥–µ–Ω–∏–µ—Ç–æ
"""

import sqlite3
from datetime import datetime, timedelta
import re
from collections import defaultdict


def analyze_scraped_data():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä–∞ –¥–∞–Ω–Ω–∏—Ç–µ –≤ SQLite –±–∞–∑–∞—Ç–∞"""
    print("=== –ê–ù–ê–õ–ò–ó –ù–ê SCRAPER –î–ê–ù–ù–ò ===")

    try:
        conn = sqlite3.connect('crypto_news.db')
        cursor = conn.cursor()

        # 1. –û–±—â–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print("\nüìä –û–ë–©–ò –°–¢–ê–¢–ò–°–¢–ò–ö–ò:")
        cursor.execute("SELECT COUNT(*) FROM articles")
        total_articles = cursor.fetchone()[0]
        print(f"   üì∞ –û–±—â–æ —Å—Ç–∞—Ç–∏–∏ –≤ –±–∞–∑–∞—Ç–∞: {total_articles}")

        cursor.execute("SELECT COUNT(*) FROM scraped_urls")
        total_urls = cursor.fetchone()[0]
        print(f"   üîó –û–±—â–æ scraped URLs: {total_urls}")

        # 2. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–∞—Ç–∞ –Ω–∞ scraping
        print("\nüìÖ –°–¢–ê–¢–ò–°–¢–ò–ö–ò –ü–û –î–ê–¢–ê –ù–ê SCRAPING:")
        cursor.execute("""
            SELECT DATE(scraped_at) as date, COUNT(*) as count,
                   AVG(content_length) as avg_length
            FROM articles 
            GROUP BY DATE(scraped_at) 
            ORDER BY date DESC
            LIMIT 7
        """)

        scraping_by_date = cursor.fetchall()
        for date, count, avg_length in scraping_by_date:
            print(f"   üìÖ {date}: {count} —Å—Ç–∞—Ç–∏–∏ (—Å—Ä–µ–¥–Ω–æ {int(avg_length)} chars)")

        # 3. –ê–Ω–∞–ª–∏–∑ –Ω–∞ URL patterns
        print("\nüîç –ê–ù–ê–õ–ò–ó –ù–ê URL PATTERNS:")
        cursor.execute("SELECT url FROM articles ORDER BY scraped_at DESC LIMIT 10")
        recent_urls = cursor.fetchall()

        url_patterns = defaultdict(int)
        dates_in_urls = []

        for (url,) in recent_urls:
            # –ò–∑–≤–ª–∏—á–∞–º–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ç–∞
            if '/markets/' in url:
                url_patterns['markets'] += 1
            elif '/policy/' in url:
                url_patterns['policy'] += 1
            elif '/tech/' in url:
                url_patterns['tech'] += 1
            elif '/business/' in url:
                url_patterns['business'] += 1
            elif '/daybook' in url:
                url_patterns['daybook'] += 1
            else:
                url_patterns['other'] += 1

            # –ò–∑–≤–ª–∏—á–∞–º–µ –¥–∞—Ç–∞—Ç–∞ –æ—Ç URL
            date_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', url)
            if date_match:
                year, month, day = date_match.groups()
                dates_in_urls.append(f"{year}-{month}-{day}")

        print("   üìã –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ç–µ 10 —Å—Ç–∞—Ç–∏–∏:")
        for category, count in url_patterns.items():
            print(f"      - {category}: {count}")

        print("   üìÖ –î–∞—Ç–∏ –≤ URL-–∞—Ç–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ç–µ 10 —Å—Ç–∞—Ç–∏–∏:")
        unique_dates = list(set(dates_in_urls))
        for date in sorted(unique_dates, reverse=True):
            count = dates_in_urls.count(date)
            print(f"      - {date}: {count} —Å—Ç–∞—Ç–∏–∏")

        # 4. –ù–∞–π-–Ω–æ–≤–∏ vs –Ω–∞–π-—Å—Ç–∞—Ä–∏ —Å—Ç–∞—Ç–∏–∏
        print("\nüïí –í–†–ï–ú–ï–ù–°–ö–ò –ê–ù–ê–õ–ò–ó:")
        cursor.execute("""
            SELECT title, url, scraped_at 
            FROM articles 
            ORDER BY scraped_at DESC 
            LIMIT 3
        """)
        newest = cursor.fetchall()

        cursor.execute("""
            SELECT title, url, scraped_at 
            FROM articles 
            ORDER BY scraped_at ASC 
            LIMIT 3
        """)
        oldest = cursor.fetchall()

        print("   üì∞ –ù–∞–π-–Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏:")
        for title, url, scraped_at in newest:
            date_in_url = extract_date_from_url(url)
            print(f"      - {title[:50]}... (scraped: {scraped_at}, URL date: {date_in_url})")

        print("   üì∞ –ù–∞–π-—Å—Ç–∞—Ä–∏ —Å—Ç–∞—Ç–∏–∏:")
        for title, url, scraped_at in oldest:
            date_in_url = extract_date_from_url(url)
            print(f"      - {title[:50]}... (scraped: {scraped_at}, URL date: {date_in_url})")

        conn.close()

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑: {e}")


def extract_date_from_url(url):
    """–ò–∑–≤–ª–∏—á–∞ –¥–∞—Ç–∞ –æ—Ç CoinDesk URL"""
    date_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', url)
    if date_match:
        year, month, day = date_match.groups()
        return f"{year}-{month}-{day}"
    return "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞"


def simulate_scraping_decision():
    """–°–∏–º—É–ª–∏—Ä–∞ –∫–∞–∫ scraper-—ä—Ç —Ä–µ—à–∞–≤–∞ –∫–æ–∏ —Å—Ç–∞—Ç–∏–∏ –¥–∞ scrape-–Ω–µ"""
    print("\n=== –°–ò–ú–£–õ–ê–¶–ò–Ø –ù–ê SCRAPING –†–ï–®–ï–ù–ò–Ø ===")

    try:
        from scraper import CoinDeskScraper

        # –°—ä–∑–¥–∞–≤–∞–º–µ scraper –±–µ–∑ database –∑–∞ —Ç–µ—Å—Ç
        scraper = CoinDeskScraper(use_database=False)
        print("üì° –°–≤—ä—Ä–∑–≤–∞–Ω–µ —Å CoinDesk...")

        # –í–∑–µ–º–∞–º–µ —Å—Ç–∞—Ç–∏–∏—Ç–µ –æ—Ç –≥–ª–∞–≤–Ω–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        article_links = scraper.get_article_links()
        print(f"üì∞ –ù–∞–º–µ—Ä–µ–Ω–∏ {len(article_links)} —Å—Ç–∞—Ç–∏–∏ –æ—Ç –≥–ª–∞–≤–Ω–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–º–µ –ø—ä—Ä–≤–∏—Ç–µ 10
        print("\nüîç –ê–ù–ê–õ–ò–ó –ù–ê –ü–™–†–í–ò–¢–ï 10 –°–¢–ê–¢–ò–ò:")
        for i, article in enumerate(article_links[:10], 1):
            url = article['url']
            title = article['title']
            date_in_url = extract_date_from_url(url)

            # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞–ª–∏ –µ –≤ –±–∞–∑–∞—Ç–∞
            is_scraped = check_if_url_scraped(url)
            status = "üü¢ –ù–û–í" if not is_scraped else "üî¥ SCRAPED"

            print(f"   {i:2d}. {status} | {date_in_url} | {title[:45]}...")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–∞—Ç–∞
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ò –ü–û –î–ê–¢–ê –ù–ê –°–¢–ê–¢–ò–ò–¢–ï:")
        date_stats = defaultdict(lambda: {'total': 0, 'new': 0, 'scraped': 0})

        for article in article_links:
            url = article['url']
            date_in_url = extract_date_from_url(url)
            is_scraped = check_if_url_scraped(url)

            date_stats[date_in_url]['total'] += 1
            if is_scraped:
                date_stats[date_in_url]['scraped'] += 1
            else:
                date_stats[date_in_url]['new'] += 1

        for date in sorted(date_stats.keys(), reverse=True):
            stats = date_stats[date]
            print(f"   üìÖ {date}: {stats['total']} –æ–±—â–æ | {stats['new']} –Ω–æ–≤–∏ | {stats['scraped']} scraped")

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å–∏–º—É–ª–∞—Ü–∏—è: {e}")


def check_if_url_scraped(url):
    """–ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ URL –µ —Å–∫—Ä–∞–ø–Ω–∞—Ç"""
    try:
        conn = sqlite3.connect('crypto_news.db')
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM scraped_urls WHERE url = ?", (url,))
        result = cursor.fetchone() is not None
        conn.close()
        return result
    except:
        return False


def recommend_scraping_strategy():
    """–ü—Ä–µ–ø–æ—Ä—ä—á–≤–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞ scraping"""
    print("\n=== –ü–†–ï–ü–û–†–™–ö–ò –ó–ê SCRAPING –°–¢–†–ê–¢–ï–ì–ò–Ø ===")

    today = datetime.now().strftime('%Y-%m-%d')
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

    print(f"üìÖ –î–Ω–µ—Å –µ: {today}")
    print(f"üìÖ –í—á–µ—Ä–∞ –±–µ—à–µ: {yesterday}")

    try:
        conn = sqlite3.connect('crypto_news.db')
        cursor = conn.cursor()

        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞–ª–∏ –∏–º–∞–º–µ —Å—Ç–∞—Ç–∏–∏ –æ—Ç –¥–Ω–µ—Å–∫–∞
        cursor.execute("""
            SELECT COUNT(*) FROM articles 
            WHERE url LIKE ? OR url LIKE ?
        """, (f'%/{today.replace("-", "/")}/%', f'%{today}%'))

        today_count = cursor.fetchone()[0]

        cursor.execute("""
            SELECT COUNT(*) FROM articles 
            WHERE url LIKE ? OR url LIKE ?
        """, (f'%/{yesterday.replace("-", "/")}/%', f'%{yesterday}%'))

        yesterday_count = cursor.fetchone()[0]

        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ò:")
        print(f"   üì∞ –°—Ç–∞—Ç–∏–∏ –æ—Ç –¥–Ω–µ—Å–∫–∞ –≤ –±–∞–∑–∞—Ç–∞: {today_count}")
        print(f"   üì∞ –°—Ç–∞—Ç–∏–∏ –æ—Ç –≤—á–µ—Ä–∞ –≤ –±–∞–∑–∞—Ç–∞: {yesterday_count}")

        # –ü—Ä–µ–ø–æ—Ä—ä–∫–∏
        print(f"\nüí° –ü–†–ï–ü–û–†–™–ö–ò:")
        if today_count == 0:
            print("   üéØ –ü—Ä–µ–ø–æ—Ä—ä—á–≤–∞–º: –°–∫—Ä–∞–ø–Ω–∏ —Å—Ç–∞—Ç–∏–∏ –æ—Ç –¥–Ω–µ—Å–∫–∞ —Å –ª–∏–º–∏—Ç 10-15")
            print("   üìù –ö–æ–º–∞–Ω–¥–∞: python run_scraper.py scrape --limit 15")
        elif today_count < 5:
            print("   üéØ –ü—Ä–µ–ø–æ—Ä—ä—á–≤–∞–º: –°–∫—Ä–∞–ø–Ω–∏ –æ—â–µ —Å—Ç–∞—Ç–∏–∏ —Å –ª–∏–º–∏—Ç 10")
            print("   üìù –ö–æ–º–∞–Ω–¥–∞: python run_scraper.py scrape --limit 10")
        else:
            print("   ‚úÖ –î–æ–±—Ä–æ –ø–æ–∫—Ä–∏—Ç–∏–µ –∑–∞ –¥–Ω–µ—Å–∫–∞, –º–æ–∂–µ—à –¥–∞ —Å–∫—Ä–∞–ø–Ω–µ—à 5-10 –Ω–æ–≤–∏")
            print("   üìù –ö–æ–º–∞–Ω–¥–∞: python run_scraper.py scrape --limit 5")

        conn.close()

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –ø—Ä–µ–ø–æ—Ä—ä–∫–∏: {e}")


def main():
    """–ì–ª–∞–≤–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîç COINDESK SCRAPER DEBUG TOOL")
    print("=" * 50)

    print("\n–ò–∑–±–µ—Ä–∏ –∞–Ω–∞–ª–∏–∑:")
    print("1. –ê–Ω–∞–ª–∏–∑ –Ω–∞ scraped –¥–∞–Ω–Ω–∏")
    print("2. –°–∏–º—É–ª–∞—Ü–∏—è –Ω–∞ scraping —Ä–µ—à–µ–Ω–∏—è")
    print("3. –ü—Ä–µ–ø–æ—Ä—ä–∫–∏ –∑–∞ scraping —Å—Ç—Ä–∞—Ç–µ–≥–∏—è")
    print("4. –í—Å–∏—á–∫–∏ –∞–Ω–∞–ª–∏–∑–∏")

    try:
        choice = input("\n–í—ä–≤–µ–¥–∏ –Ω–æ–º–µ—Ä (1-4): ").strip()

        if choice == "1":
            analyze_scraped_data()
        elif choice == "2":
            simulate_scraping_decision()
        elif choice == "3":
            recommend_scraping_strategy()
        elif choice == "4":
            analyze_scraped_data()
            simulate_scraping_decision()
            recommend_scraping_strategy()
        else:
            print("‚ùå –ù–µ–≤–∞–ª–∏–¥–µ–Ω –∏–∑–±–æ—Ä")

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –ü—Ä–µ–∫—Ä–∞—Ç–µ–Ω–æ –æ—Ç –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è")
    except Exception as e:
        print(f"\n‚ùå –ì—Ä–µ—à–∫–∞: {e}")


if __name__ == "__main__":
    main()
