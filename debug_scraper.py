import requests
from bs4 import BeautifulSoup
from config import REQUEST_HEADERS


def debug_article_structure(url):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä–∞ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–∞—Ç–∞ –Ω–∞ —Å—Ç–∞—Ç–∏—è –∑–∞ –¥–∞ –Ω–∞–º–µ—Ä–∏–º –ø—Ä–∞–≤–∏–ª–Ω–∏—Ç–µ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏"""
    print(f"üîç Debugging HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–∞: {url}")

    try:
        session = requests.Session()
        session.headers.update(REQUEST_HEADERS)

        response = session.get(url, timeout=15)
        response.raise_for_status()

        # –í–∞–∂–Ω–æ: –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–∞–Ω–µ –Ω–∞ compressed content
        if response.encoding is None:
            response.encoding = 'utf-8'

        # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ response.text –≤–º–µ—Å—Ç–æ response.content –∑–∞ –ø—Ä–∞–≤–∏–ª–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–∞–Ω–µ
        soup = BeautifulSoup(response.text, 'html.parser')

        print(f"üìä Status –∫–æ–¥: {response.status_code}")
        print(f"üìÑ Encoding: {response.encoding}")

        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –∑–∞–≥–ª–∞–≤–∏–µ—Ç–æ
        print("\n=== –ó–ê–ì–õ–ê–í–ò–ï ===")
        possible_titles = [
            soup.find('h1'),
            soup.find('title'),
            soup.select_one('[data-module="ArticleHeader"] h1'),
            soup.select_one('.headline'),
        ]

        for i, title in enumerate(possible_titles, 1):
            if title:
                print(f"  {i}. {title.name}: {title.get_text().strip()[:100]}")
            else:
                print(f"  {i}. None")

        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ
        print("\n=== –°–™–î–™–†–ñ–ê–ù–ò–ï ===")

        # –¢—ä—Ä—Å–∏–º —Ä–∞–∑–ª–∏—á–Ω–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –∑–∞ —Å—Ç–∞—Ç–∏–∏
        content_selectors = [
            '[data-module="ArticleBody"]',
            '.article-content',
            '.post-content',
            'article',
            '.entry-content',
            'main article',
            '.content'
        ]

        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                content = elements[0].get_text().strip()
                print(f"‚úÖ {selector}: {len(content)} chars - {content[:150]}...")

                # –ü–æ–∫–∞–∑–≤–∞–º–µ –∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏—Ç–µ
                paragraphs = elements[0].find_all('p')
                print(f"   üìù –ü–∞—Ä–∞–≥—Ä–∞—Ñ–∏: {len(paragraphs)}")
                if paragraphs:
                    print(f"   üìù –ü—ä—Ä–≤–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ: {paragraphs[0].get_text().strip()[:100]}...")
            else:
                print(f"‚ùå {selector}: –ù–µ –Ω–∞–º–µ—Ä–µ–Ω")

        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –≤—Å–∏—á–∫–∏ <p> tags
        all_paragraphs = soup.find_all('p')
        print(f"\nüì∞ –û–±—â–æ <p> tags: {len(all_paragraphs)}")

        # –ü–æ–∫–∞–∑–≤–∞–º–µ –ø—ä—Ä–≤–∏—Ç–µ 5 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
        print("\n=== –ü–™–†–í–ò 5 –ü–ê–†–ê–ì–†–ê–§–ê ===")
        for i, p in enumerate(all_paragraphs[:5], 1):
            text = p.get_text().strip()
            if text:
                print(f"  {i}. ({len(text)} chars) {text[:100]}...")
            else:
                print(f"  {i}. (–ø—Ä–∞–∑–µ–Ω)")

        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞—Ç–∞—Ç–∞
        print("\n=== –î–ê–¢–ê ===")
        date_selectors = [
            'time[datetime]',
            '.article-date',
            '.post-date',
            '[data-timestamp]',
            'time'
        ]

        for selector in date_selectors:
            date_element = soup.select_one(selector)
            if date_element:
                print(
                    f"‚úÖ {selector}: {date_element.get('datetime', '–ù—è–º–∞ datetime')} | {date_element.get_text().strip()}")
            else:
                print(f"‚ùå {selector}: –ù–µ –Ω–∞–º–µ—Ä–µ–Ω")

        # –ó–∞–ø–∞–∑–≤–∞–º–µ HTML –∑–∞ debugging (–ø—ä—Ä–≤–∏—Ç–µ 10000 —Å–∏–º–≤–æ–ª–∞)
        print(f"\n=== HTML PREVIEW ===")
        html_preview = str(soup)[:2000]
        print(html_preview)

        return True

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ debugging: {str(e)}")
        return False


if __name__ == "__main__":
    # –¢–µ—Å—Ç–≤–∞–º–µ —Å –∞–∫—Ç—É–∞–ª–Ω–∞ —Å—Ç–∞—Ç–∏—è
    test_url = "https://www.coindesk.com/markets/2025/06/09/bitwise-proshares-file-for-etfs-tracking-soaring-circle-shares"
    debug_article_structure(test_url)
