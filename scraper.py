#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –Ω–∞ –ø–æ–¥–æ–±—Ä–µ–Ω–∏—è CoinDesk scraper
–ó–∞–º–µ–Ω–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª–Ω–∏—è scraper.py —Å –Ω–æ–≤–∏—è –∫–æ–¥ –∑–∞ –¥–∞ —Ç–µ—Å—Ç–≤–∞—à
"""

import sys
import time
from pathlib import Path

# –î–æ–±–∞–≤–∏ current directory –∫—ä–º path –∑–∞ import
sys.path.append(str(Path(__file__).parent))


def test_with_existing_project():
    """–¢–µ—Å—Ç–≤–∞ —Å —ñ—Å–Ω—É–≤–∞—â–∏—è –ø—Ä–æ–µ–∫—Ç (–∑–∞–º–µ–Ω–∏ scraper.py)"""
    print("=== –¢–ï–°–¢ –ù–ê –ü–û–î–û–ë–†–ï–ù–ò–Ø SCRAPER ===")
    print("‚ö†Ô∏è  –ó–∞ —Ç–æ–∑–∏ —Ç–µ—Å—Ç —Ç—Ä—è–±–≤–∞ –¥–∞ –∑–∞–º–µ–Ω–∏—à scraper.py —Å –Ω–æ–≤–∏—è –∫–æ–¥!")
    print()

    try:
        # Import –Ω–∞ –ø–æ–¥–æ–±—Ä–µ–Ω–∏—è scraper
        from scraper import CoinDeskScraper

        print("‚úÖ Scraper imported —É—Å–ø–µ—à–Ω–æ")

        # –¢–µ—Å—Ç 1: URL validation
        print("\nüîç –¢–µ—Å—Ç 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ URL detection...")
        scraper = CoinDeskScraper(use_database=False)
        article_links = scraper.get_article_links()

        print(f"üìä –ù–∞–º–µ—Ä–µ–Ω–∏ {len(article_links)} —Å—Ç–∞—Ç–∏–∏")

        if len(article_links) == 0:
            print("‚ùå –ù—è–º–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —Å—Ç–∞—Ç–∏–∏ - –ø—Ä–æ–±–ª–µ–º —Å URL detection")
            return False

        # –ü–æ–∫–∞–∑–≤–∞–º–µ –ø—ä—Ä–≤–∏—Ç–µ 3
        print("\nüìã –ü—ä—Ä–≤–∏ 3 —Å—Ç–∞—Ç–∏–∏:")
        for i, link in enumerate(article_links[:3], 1):
            print(f"  {i}. {link['title'][:60]}...")
            print(f"     ‚Üí {link['url']}")

        # –¢–µ—Å—Ç 2: Single article scraping
        print(f"\nüîç –¢–µ—Å—Ç 2: Scraping –Ω–∞ –ø—ä—Ä–≤–∞—Ç–∞ —Å—Ç–∞—Ç–∏—è...")
        first_article_url = article_links[0]['url']
        article_data = scraper.scrape_single_article(first_article_url)

        if article_data:
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—è!")
            print(f"   üìÑ –ó–∞–≥–ª–∞–≤–∏–µ: {article_data['title']}")
            print(f"   üìä –î—ä–ª–∂–∏–Ω–∞: {article_data['content_length']} chars")
            print(f"   üë§ –ê–≤—Ç–æ—Ä: {article_data['author']}")
            print(f"   üìÖ –î–∞—Ç–∞: {article_data['date']}")
            print(f"   üìù –ü—ä—Ä–≤–∏ 200 chars: {article_data['content'][:200]}...")
        else:
            print("‚ùå –ù–µ—É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏—è—Ç–∞")
            return False

        # –¢–µ—Å—Ç 3: Multiple articles
        print(f"\nüîç –¢–µ—Å—Ç 3: Scraping –Ω–∞ 5 —Å—Ç–∞—Ç–∏–∏...")
        articles = scraper.scrape_multiple_articles(max_articles=5, save_to_db=False)

        print(f"\nüìä –†–ï–ó–£–õ–¢–ê–¢–ò:")
        print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏: {len(articles)} —Å—Ç–∞—Ç–∏–∏")
        print(f"   üìà Success rate: {len(articles)}/5 = {(len(articles) / 5) * 100:.1f}%")

        if len(articles) >= 3:
            print("üéâ –¢–ï–°–¢–™–¢ –£–°–ü–ï–®–ï–ù! Scraper-—ä—Ç —Ä–∞–±–æ—Ç–∏ –¥–æ–±—Ä–µ!")
            print("\n–ò–∑–≤–ª–µ—á–µ–Ω–∏ —Å—Ç–∞—Ç–∏–∏:")
            for i, article in enumerate(articles, 1):
                print(f"  {i}. {article['title'][:50]}... ({article['content_length']} chars)")
            return True
        else:
            print("‚ö†Ô∏è  Scraper-—ä—Ç —Ä–∞–±–æ—Ç–∏, –Ω–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç—Ç–∞ –µ –Ω–∏—Å–∫–∞")
            return False

    except ImportError as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ import: {e}")
        print("üí° –£–≤–µ—Ä–µ—Ç–µ —Å–µ, —á–µ —Å—Ç–µ –∑–∞–º–µ–Ω–∏–ª–∏ scraper.py —Å –Ω–æ–≤–∏—è –∫–æ–¥")
        return False
    except Exception as e:
        print(f"‚ùå –ù–µ–æ—á–∞–∫–≤–∞–Ω–∞ –≥—Ä–µ—à–∫–∞: {e}")
        return False


def run_quick_debug():
    """–ë—ä—Ä–∑ debug –Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ –±–µ–∑ –∑–∞–º—è–Ω–∞ –Ω–∞ —Ñ–∞–π–ª–æ–≤–µ"""
    print("=== –ë–™–†–ó DEBUG –ù–ê –ü–†–û–ë–õ–ï–ú–ê ===")

    import requests
    from bs4 import BeautifulSoup

    # –¢–µ—Å—Ç–≤–∞–º–µ –¥–∏—Ä–µ–∫—Ç–Ω–æ –µ–¥–Ω–∞ —Å—Ç–∞—Ç–∏—è
    test_url = "https://www.coindesk.com/markets/2025/06/09/bitwise-proshares-file-for-etfs-tracking-soaring-circle-shares"

    print(f"üîç –¢–µ—Å—Ç–≤–∞–º –¥–∏—Ä–µ–∫—Ç–Ω–æ: {test_url}")

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

        response = requests.get(test_url, headers=headers, timeout=15)
        print(f"üìä Status: {response.status_code}")

        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')

            # –¢–µ—Å—Ç 1: Title
            h1 = soup.find('h1')
            title = h1.get_text().strip() if h1 else "No H1"
            print(f"üìÑ Title: {title}")

            # –¢–µ—Å—Ç 2: Paragraphs
            all_p = soup.find_all('p')
            print(f"üìù –ù–∞–º–µ—Ä–µ–Ω–∏ {len(all_p)} <p> tags")

            # –ü–æ–∫–∞–∑–≤–∞–º–µ –ø—ä—Ä–≤–∏—Ç–µ 3 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
            meaningful_p = []
            for p in all_p[:10]:  # –ü—ä—Ä–≤–∏—Ç–µ 10
                text = p.get_text().strip()
                if len(text) > 30:
                    meaningful_p.append(text)

            print(f"üìù –°–º–∏—Å–ª–µ–Ω–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏: {len(meaningful_p)}")

            if meaningful_p:
                print("üìù –ü—ä—Ä–≤–∏ 3 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞:")
                for i, para in enumerate(meaningful_p[:3], 1):
                    print(f"  {i}. {para[:100]}...")

                total_content = '\n\n'.join(meaningful_p)
                print(f"üìä –û–±—â–æ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ: {len(total_content)} chars")

                if len(total_content) > 200:
                    print("‚úÖ –î–æ—Å—Ç–∞—Ç—ä—á–Ω–æ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ –∑–∞ –∏–∑–≤–ª–∏—á–∞–Ω–µ!")
                    return True
                else:
                    print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ")
            else:
                print("‚ùå –ù—è–º–∞ —Å–º–∏—Å–ª–µ–Ω–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏")

        else:
            print(f"‚ùå HTTP –≥—Ä–µ—à–∫–∞: {response.status_code}")

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞: {e}")

    return False


def analyze_current_scraper_issue():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å —Ç–µ–∫—É—â–∏—è scraper"""
    print("=== –ê–ù–ê–õ–ò–ó –ù–ê –¢–ï–ö–£–©–ò–Ø SCRAPER –ü–†–û–ë–õ–ï–ú ===")

    try:
        # –û–ø–∏—Ç–≤–∞–º–µ –¥–∞ –Ω–∞–º–µ—Ä–∏–º –ø—Ä–æ–±–ª–µ–º–∞ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–Ω–∏—è scraper
        print("üîç –¢—ä—Ä—Å—è config.py...")
        with open('config.py', 'r', encoding='utf-8') as f:
            config_content = f.read()

        print("‚úÖ config.py –Ω–∞–º–µ—Ä–µ–Ω")

        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ HTML_SELECTORS
        if 'HTML_SELECTORS' in config_content:
            print("‚úÖ HTML_SELECTORS –Ω–∞–º–µ—Ä–µ–Ω–∏ –≤ config")

            # –ò–∑–≤–ª–∏—á–∞–º–µ article_content —Å–µ–ª–µ–∫—Ç–æ—Ä–∏—Ç–µ
            import re
            content_match = re.search(r"'article_content':\s*\[(.*?)\]", config_content, re.DOTALL)
            if content_match:
                selectors = content_match.group(1)
                print(f"üìã –¢–µ–∫—É—â–∏ content —Å–µ–ª–µ–∫—Ç–æ—Ä–∏: {selectors}")

        print("\nüí° –ü–†–û–ë–õ–ï–ú–™–¢: –°–µ–ª–µ–∫—Ç–æ—Ä–∏—Ç–µ –≤ config.py –Ω–µ —Ä–∞–±–æ—Ç—è—Ç —Å –Ω–æ–≤–∞—Ç–∞ CoinDesk —Å—Ç—Ä—É–∫—Ç—É—Ä–∞")
        print("üí° –†–ï–®–ï–ù–ò–ï: –¢—Ä—è–±–≤–∞ –¥–∞ –æ–±–Ω–æ–≤–∏–º scraper.py —Å –Ω–æ–≤–∞—Ç–∞ –ª–æ–≥–∏–∫–∞")

    except FileNotFoundError:
        print("‚ùå config.py –Ω–µ –µ –Ω–∞–º–µ—Ä–µ–Ω")
    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑: {e}")


def provide_solution_steps():
    """–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—è —Å—Ç—ä–ø–∫–∏ –∑–∞ —Ä–µ—à–µ–Ω–∏–µ"""
    print("\n" + "=" * 60)
    print("üöÄ –°–¢–™–ü–ö–ò –ó–ê –†–ï–®–ê–í–ê–ù–ï –ù–ê –ü–†–û–ë–õ–ï–ú–ê")
    print("=" * 60)

    print("""
üîß –°–¢–™–ü–ö–ê 1: –ó–∞–º–µ–Ω–∏ scraper.py
   - –ö–æ–ø–∏—Ä–∞–π –Ω–æ–≤–∏—è –∫–æ–¥ –æ—Ç artifacts
   - –ó–∞–º–µ—Å—Ç–∏ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏—è scraper.py —Ñ–∞–π–ª

üîß –°–¢–™–ü–ö–ê 2: –¢–µ—Å—Ç–≤–∞–π
   - –ò–∑–ø—ä–ª–Ω–∏: python -c "from scraper import CoinDeskScraper; s=CoinDeskScraper(False); print(len(s.get_article_links()))"

üîß –°–¢–™–ü–ö–ê 3: –ü—ä–ª–µ–Ω —Ç–µ—Å—Ç
   - –ò–∑–ø—ä–ª–Ω–∏: python run_scraper.py scrape --limit 5 --verbose

üîß –°–¢–™–ü–ö–ê 4: –ü—Ä–æ–≤–µ—Ä–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∞
   - –¢—Ä—è–±–≤–∞ –¥–∞ –≤–∏–¥–∏—à –ø–æ–≤–µ—á–µ –æ—Ç 3 —Å—Ç–∞—Ç–∏–∏
   - Success rate —Ç—Ä—è–±–≤–∞ –¥–∞ –µ >80%
    """)

    print("üí° –ê–∫–æ –∏–º–∞ –ø—Ä–æ–±–ª–µ–º–∏, –∏–∑–ø—Ä–∞—Ç–∏ –≥—Ä–µ—à–∫–∏—Ç–µ –∏ —â–µ –ø–æ–º–æ–≥–Ω–∞!")


if __name__ == "__main__":
    print("üß™ COINDESK SCRAPER –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê")
    print("=" * 50)

    # –ò–∑–±–æ—Ä –Ω–∞ —Ç–µ—Å—Ç
    print("\n–í—ä–∑–º–æ–∂–Ω–∏ —Ç–µ—Å—Ç–æ–≤–µ:")
    print("1. –ë—ä—Ä–∑ debug (–±–µ–∑ –ø—Ä–æ–º–µ–Ω–∏ –Ω–∞ —Ñ–∞–π–ª–æ–≤–µ)")
    print("2. –¢–µ—Å—Ç —Å –ø–æ–¥–æ–±—Ä–µ–Ω–∏—è scraper (–∏–∑–∏—Å–∫–≤–∞ –∑–∞–º—è–Ω–∞ –Ω–∞ scraper.py)")
    print("3. –ê–Ω–∞–ª–∏–∑ –Ω–∞ —Ç–µ–∫—É—â–∏—è –ø—Ä–æ–±–ª–µ–º")
    print("4. –°—Ç—ä–ø–∫–∏ –∑–∞ —Ä–µ—à–µ–Ω–∏–µ")

    try:
        choice = input("\n–ò–∑–±–µ—Ä–∏ —Ç–µ—Å—Ç (1-4): ").strip()

        if choice == "1":
            success = run_quick_debug()
        elif choice == "2":
            success = test_with_existing_project()
        elif choice == "3":
            analyze_current_scraper_issue()
            success = True
        elif choice == "4":
            provide_solution_steps()
            success = True
        else:
            print("‚ùå –ù–µ–≤–∞–ª–∏–¥–µ–Ω –∏–∑–±–æ—Ä")
            success = False

        if success:
            print("\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤—ä—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        else:
            print("\n‚ùå –¢–µ—Å—Ç –Ω–µ—É—Å–ø–µ—à–µ–Ω")

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –ü—Ä–µ–∫—Ä–∞—Ç–µ–Ω–æ –æ—Ç –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è")
    except Exception as e:
        print(f"\n‚ùå –ì—Ä–µ—à–∫–∞: {e}")
