import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
from urllib.parse import urljoin
import re

from config import (
    COINDESK_MAIN_PAGE,
    REQUEST_HEADERS,
    NEWS_URL_PATTERNS,
    SCRAPING_CONFIG,
    HTML_SELECTORS,
    is_valid_article_url,
    get_full_url
)
from database import DatabaseManager


class CoinDeskScraper:
    def __init__(self, use_database=True):
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ CoinDesk Scraper...")
        self.session = requests.Session()

        # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ –ø–æ-–ø—Ä–æ—Å—Ç–∏ headers
        simple_headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
        }
        self.session.headers.update(simple_headers)
        self.scraped_urls = set()

        # Database –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
        self.use_database = use_database
        if use_database:
            self.db = DatabaseManager()
        else:
            self.db = None

        print("‚úÖ Scraper –≥–æ—Ç–æ–≤!")

    def get_article_links(self):
        """–ù–∞–º–∏—Ä–∞ –≤—Å–∏—á–∫–∏ –ª–∏–Ω–∫–æ–≤–µ –∫—ä–º —Å—Ç–∞—Ç–∏–∏ –æ—Ç –≥–ª–∞–≤–Ω–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
        print("üîç –¢—ä—Ä—Å–µ–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏–∏ –≤ –≥–ª–∞–≤–Ω–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞...")

        try:
            response = self.session.get(
                COINDESK_MAIN_PAGE,
                timeout=SCRAPING_CONFIG['request_timeout']
            )
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # –¢—ä—Ä—Å–∏–º –≤—Å–∏—á–∫–∏ <a> —Ç–∞–≥–æ–≤–µ —Å href
            article_links = []
            all_links = soup.find_all('a', href=True)

            for link in all_links:
                href = link['href']

                # –ü—Ä–∞–≤–∏–º –ø—ä–ª–µ–Ω URL
                if href.startswith('/'):
                    full_url = f"https://www.coindesk.com{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue

                # –ü–æ–¥–æ–±—Ä–µ–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ URL
                if self._is_valid_article_url_improved(href):
                    title = self._extract_link_title(link)
                    if title and len(title) > 15:
                        article_links.append({
                            'url': full_url,
                            'title': title,
                            'href': href
                        })

            # –ü—Ä–µ–º–∞—Ö–≤–∞–º–µ –¥—É–±–ª–∏—Ä–∞–Ω–∏—Ç–µ URLs
            unique_articles = []
            seen_urls = set()
            for article in article_links:
                if article['url'] not in seen_urls:
                    unique_articles.append(article)
                    seen_urls.add(article['url'])

            print(f"üì∞ –ù–∞–º–µ—Ä–µ–Ω–∏ {len(unique_articles)} —É–Ω–∏–∫–∞–ª–Ω–∏ —Å—Ç–∞—Ç–∏–∏")

            # DEBUG –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            print("üîç –ü—ä—Ä–≤–∏ 5 —Å—Ç–∞—Ç–∏–∏ –∑–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞:")
            for i, article in enumerate(unique_articles[:5], 1):
                print(f"  {i}. {article['title'][:60]}...")

            return unique_articles

        except Exception as e:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–∏—á–∞–Ω–µ—Ç–æ –Ω–∞ –ª–∏–Ω–∫–æ–≤–µ: {str(e)}")
            return []

    def _is_valid_article_url_improved(self, href):
        """–ü–æ–¥–æ–±—Ä–µ–Ω–∞ –ª–æ–≥–∏–∫–∞ –∑–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ article URLs"""

        if not href or href in ['#', '/', '']:
            return False

        if href.startswith('http') and 'coindesk.com' not in href:
            return False

        if href.startswith('#') or href.startswith('mailto:') or href.startswith('tel:'):
            return False

        # –ò–∑–∫–ª—é—á–≤–∞–º–µ —Å–∏—Å—Ç–µ–º–Ω–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∏
        exclude_patterns = [
            '/newsletters/', '/podcasts/', '/events/', '/about/', '/careers/',
            '/advertise/', '/price/', '/author/', '/tag/', '/sponsored-content/',
            '/_next/', '/api/'
        ]

        for pattern in exclude_patterns:
            if pattern in href:
                return False

        # –¢—ä—Ä—Å–∏–º —Å—Ç–∞—Ç–∏–∏ —Å –≥–æ–¥–∏–Ω–∞ –≤ URL-–∞ –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        date_patterns = [
            r'/\d{4}/\d{2}/\d{2}/',  # /2025/06/09/
            r'/2025/', r'/2024/'
        ]

        has_date = any(re.search(pattern, href) for pattern in date_patterns)

        news_categories = ['/markets/', '/policy/', '/tech/', '/business/', '/layer2/', '/web3/']
        has_category = any(category in href for category in news_categories)

        return has_date or has_category

    def _extract_link_title(self, link):
        """–ò–∑–≤–ª–∏—á–∞ –∑–∞–≥–ª–∞–≤–∏–µ—Ç–æ –æ—Ç link element"""
        text = link.get_text().strip()
        if text and len(text) > 5:
            return ' '.join(text.split())

        title_attr = link.get('title', '').strip()
        if title_attr:
            return title_attr

        aria_label = link.get('aria-label', '').strip()
        if aria_label:
            return aria_label

        return ""

    def scrape_single_article(self, article_url):
        """–ò–∑–≤–ª–∏—á–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–∞ –µ–¥–Ω–∞ —Å—Ç–∞—Ç–∏—è"""
        print(f"üìÑ Scraping —Å—Ç–∞—Ç–∏—è: {article_url}")

        try:
            time.sleep(SCRAPING_CONFIG['delay_between_requests'])

            response = self.session.get(
                article_url,
                timeout=SCRAPING_CONFIG['request_timeout']
            )
            response.raise_for_status()

            content_text = response.content.decode('utf-8', errors='ignore')
            soup = BeautifulSoup(content_text, 'html.parser')

            # –ò–∑–≤–ª–∏—á–∞–º–µ –¥–∞–Ω–Ω–∏—Ç–µ
            title = self._extract_title_improved(soup)
            content = self._extract_content_improved(soup)
            date = self._extract_date_improved(soup)
            author = self._extract_author_improved(soup)

            # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥—ä–ª–∂–∏–Ω–∞—Ç–∞
            if len(content) < SCRAPING_CONFIG['min_article_length']:
                print(f"‚ö†Ô∏è  –°—Ç–∞—Ç–∏—è—Ç–∞ –µ —Ç–≤—ä—Ä–¥–µ –∫—Ä–∞—Ç–∫–∞ ({len(content)} chars)")
                print(f"üîç DEBUG –ø—ä—Ä–≤–∏ 200 chars: {content[:200]}")
                return None

            article_data = {
                'url': article_url,
                'title': title,
                'content': content,
                'date': date,
                'author': author,
                'scraped_at': datetime.now(),
                'content_length': len(content)
            }

            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—è: {title[:50]}... ({len(content)} chars)")
            return article_data

        except Exception as e:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ scraping –Ω–∞ {article_url}: {str(e)}")
            return None

    def _extract_title_improved(self, soup):
        """–ü–æ–¥–æ–±—Ä–µ–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –∑–∞–≥–ª–∞–≤–∏–µ—Ç–æ"""

        # h1 tag –ø—ä—Ä–≤–æ
        h1_tags = soup.find_all('h1')
        for h1 in h1_tags:
            text = h1.get_text().strip()
            if text and len(text) > 10:
                return text

        # meta og:title
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title['content'].strip()

        # title tag
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
            title = re.sub(r'\s*\|\s*CoinDesk.*$', '', title)
            if title:
                return title

        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –∑–∞–≥–ª–∞–≤–∏–µ"

    def _extract_content_improved(self, soup):
        """–ü–æ–¥–æ–±—Ä–µ–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ"""

        print("üîç –ó–∞–ø–æ—á–≤–∞–º –ø–æ–¥–æ–±—Ä–µ–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ...")

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: Main containers
        main_selectors = ['main', 'article', '[role="main"]', '.article-content', '.post-content']

        for selector in main_selectors:
            container = soup.select_one(selector)
            if container:
                print(f"‚úÖ –ù–∞–º–µ—Ä–µ–Ω main container: {selector}")
                paragraphs = container.find_all('p')
                content = self._process_paragraphs(paragraphs)
                if len(content) > 200:
                    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏ {len(content)} chars –æ—Ç {selector}")
                    return content

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –í—Å–∏—á–∫–∏ <p> tags
        print("üîç –¢—ä—Ä—Å—è –≤—Å–∏—á–∫–∏ <p> tags...")
        all_paragraphs = soup.find_all('p')
        print(f"üìä –ù–∞–º–µ—Ä–µ–Ω–∏ {len(all_paragraphs)} –æ–±—â–æ <p> tags")

        if all_paragraphs:
            content = self._process_paragraphs(all_paragraphs)
            if len(content) > 100:
                print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏ {len(content)} chars –æ—Ç –≤—Å–∏—á–∫–∏ <p> tags")
                return content

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: Div containers
        print("üîç –¢—ä—Ä—Å—è div containers —Å —Ç–µ–∫—Å—Ç...")
        text_divs = soup.find_all('div')
        meaningful_text = []

        for div in text_divs:
            direct_text = div.get_text().strip()
            if 50 < len(direct_text) < 1000:
                meaningful_text.append(direct_text)

        if meaningful_text:
            content = '\n\n'.join(meaningful_text[:10])
            if len(content) > 100:
                print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏ {len(content)} chars –æ—Ç div containers")
                return content

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: Fallback
        print("üîç Fallback: –í–∑–µ–º–∞–º –≤—Å–∏—á–∫–æ –æ—Ç body...")
        body = soup.find('body')
        if body:
            for script in body(["script", "style", "nav", "header", "footer"]):
                script.decompose()

            body_text = body.get_text()
            lines = [line.strip() for line in body_text.split('\n') if line.strip()]
            content = '\n'.join(lines[:50])

            if len(content) > 100:
                print(f"‚úÖ Fallback –∏–∑–≤–ª–µ—á–µ–Ω–∏ {len(content)} chars –æ—Ç body")
                return content

        print("‚ùå –ù–µ —É—Å–ø—è—Ö –¥–∞ –∏–∑–≤–ª–µ–∫–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ")
        return "–°—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–µ –º–æ–∂–µ –¥–∞ –±—ä–¥–µ –∏–∑–≤–ª–µ—á–µ–Ω–æ"

    def _process_paragraphs(self, paragraphs):
        """–û–±—Ä–∞–±–æ—Ç–≤–∞ —Å–ø–∏—Å—ä–∫ –æ—Ç <p> tags"""
        meaningful_paragraphs = []

        for p in paragraphs:
            text = p.get_text().strip()
            if self._is_meaningful_paragraph(text):
                meaningful_paragraphs.append(text)

        return '\n\n'.join(meaningful_paragraphs)

    def _is_meaningful_paragraph(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –µ —Å–º–∏—Å–ª–µ–Ω"""
        if len(text) < 20:
            return False

        exclude_phrases = [
            'Sign up', 'Subscribe', 'Newsletter', 'See all newsletters',
            'Don\'t miss', 'By signing up', 'privacy policy', 'terms of use',
            'Cookie', 'Advertisement', 'Sponsored', 'Follow us', 'Share this',
            'Read more', 'Click here', 'Download', 'Watch', 'Listen'
        ]

        text_lower = text.lower()
        for phrase in exclude_phrases:
            if phrase.lower() in text_lower:
                return False

        if text.count('.') < 1:
            return False

        return True

    def _extract_date_improved(self, soup):
        """–ü–æ–¥–æ–±—Ä–µ–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –¥–∞—Ç–∞—Ç–∞"""

        # meta published_time
        published_meta = soup.find('meta', property='article:published_time')
        if published_meta and published_meta.get('content'):
            try:
                date_str = published_meta['content']
                return datetime.fromisoformat(date_str.replace('Z', '+00:00')).strftime('%Y-%m-%d')
            except:
                pass

        # time tags
        time_tags = soup.find_all('time')
        for time_tag in time_tags:
            datetime_attr = time_tag.get('datetime')
            if datetime_attr:
                try:
                    return datetime.fromisoformat(datetime_attr.replace('Z', '+00:00')).strftime('%Y-%m-%d')
                except:
                    pass

        # –û—Ç URL-–∞
        canonical = soup.find('link', rel='canonical')
        if canonical:
            url_date_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', canonical.get('href', ''))
            if url_date_match:
                year, month, day = url_date_match.groups()
                return f"{year}-{month}-{day}"

        return datetime.now().strftime('%Y-%m-%d')

    def _extract_author_improved(self, soup):
        """–ü–æ–¥–æ–±—Ä–µ–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –∞–≤—Ç–æ—Ä–∞"""

        # meta author
        author_meta = soup.find('meta', attrs={'name': 'author'})
        if author_meta and author_meta.get('content'):
            return author_meta['content'].strip()

        # author links
        author_selectors = [
            'a[href*="/author/"]', '.author-name', '.byline',
            '[data-author]', '.post-author'
        ]

        for selector in author_selectors:
            author_element = soup.select_one(selector)
            if author_element:
                author_text = author_element.get_text().strip()
                if author_text and len(author_text) < 100:
                    return author_text

        return "–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω –∞–≤—Ç–æ—Ä"

    def scrape_multiple_articles(self, max_articles=None, save_to_db=True):
        """Scrape-–≤–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–∏–∏"""
        if max_articles is None:
            max_articles = SCRAPING_CONFIG['max_articles_per_session']

        print(f"üéØ –ó–∞–ø–æ—á–≤–∞–º scraping –Ω–∞ –º–∞–∫—Å–∏–º—É–º {max_articles} —Å—Ç–∞—Ç–∏–∏...")

        # –ü–æ–ª—É—á–∞–≤–∞–º–µ –ª–∏–Ω–∫–æ–≤–µ—Ç–µ
        article_links = self.get_article_links()

        if not article_links:
            print("‚ùå –ù–µ —Å–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —Å—Ç–∞—Ç–∏–∏ –∑–∞ scraping")
            return []

        # Database —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ
        if self.db and save_to_db:
            print("üîç –ü—Ä–æ–≤–µ—Ä—è–≤–∞–Ω–µ –∑–∞ –¥—É–±–ª–∏—Ä–∞—â–∏ —Å–µ URLs...")
            new_article_links = []
            skipped_count = 0

            for link_info in article_links:
                url = link_info['url']
                if not self.db.is_url_scraped_before(url):
                    new_article_links.append(link_info)
                else:
                    skipped_count += 1
                    self.db.record_scraped_url(url)

            print(f"üìä {len(new_article_links)} –Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏, {skipped_count} –≤–µ—á–µ scraped")
            article_links = new_article_links

        # –û–≥—Ä–∞–Ω–∏—á–∞–≤–∞–º–µ –±—Ä–æ—è
        article_links = article_links[:max_articles]

        if not article_links:
            print("‚ÑπÔ∏è –í—Å–∏—á–∫–∏ —Å—Ç–∞—Ç–∏–∏ —Å–∞ –≤–µ—á–µ scraped")
            return []

        # Scraping
        scraped_articles = []
        successful_count = 0
        failed_count = 0

        for i, link_info in enumerate(article_links, 1):
            url = link_info['url']
            print(f"\n[{i}/{len(article_links)}] {link_info['title'][:60]}...")

            article_data = self.scrape_single_article(url)
            if article_data:
                scraped_articles.append(article_data)
                successful_count += 1

                if self.db and save_to_db:
                    self.db.save_article(article_data)
            else:
                failed_count += 1
                print(f"‚ùå –ù–µ—É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏—è {i}")

            if i % 5 == 0:
                print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å: {i}/{len(article_links)} —Å—Ç–∞—Ç–∏–∏ –æ–±—Ä–∞–±–æ—Ç–µ–Ω–∏")
                print(f"    ‚úÖ –£—Å–ø–µ—à–Ω–∏: {successful_count}, ‚ùå –ù–µ—É—Å–ø–µ—à–Ω–∏: {failed_count}")

        print(f"\nüéâ Scraping –∑–∞–≤—ä—Ä—à–µ–Ω!")
        print(f"üìä –§–∏–Ω–∞–ª–µ–Ω —Ä–µ–∑—É–ª—Ç–∞—Ç: {successful_count} —É—Å–ø–µ—à–Ω–∏, {failed_count} –Ω–µ—É—Å–ø–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏")

        if self.db and save_to_db:
            stats = self.db.get_database_stats()
            print(
                f"üìä Database —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {stats['total_articles']} –æ–±—â–æ —Å—Ç–∞—Ç–∏–∏, {stats['unprocessed_articles']} –∑–∞ –∞–Ω–∞–ª–∏–∑")

        return scraped_articles


def test_single_article():
    """–¢–µ—Å—Ç–≤–∞ scraping –Ω–∞ –µ–¥–Ω–∞ —Å—Ç–∞—Ç–∏—è"""
    scraper = CoinDeskScraper(use_database=False)

    test_url = "https://www.coindesk.com/markets/2025/06/09/bitwise-proshares-file-for-etfs-tracking-soaring-circle-shares"

    print(f"\nüß™ –¢–µ—Å—Ç–≤–∞–Ω–µ –Ω–∞ scraping –Ω–∞ –µ–¥–Ω–∞ —Å—Ç–∞—Ç–∏—è...")
    article = scraper.scrape_single_article(test_url)

    if article:
        print(f"\n‚úÖ –£–°–ü–ï–•! –ò–∑–≤–ª–µ—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—è:")
        print(f"   üìÑ –ó–∞–≥–ª–∞–≤–∏–µ: {article['title']}")
        print(f"   üìÖ –î–∞—Ç–∞: {article['date']}")
        print(f"   üë§ –ê–≤—Ç–æ—Ä: {article['author']}")
        print(f"   üìä –î—ä–ª–∂–∏–Ω–∞: {article['content_length']} —Å–∏–º–≤–æ–ª–∞")
        print(f"   üìù –ü—ä—Ä–≤–∏ 300 —Å–∏–º–≤–æ–ª–∞: {article['content'][:300]}...")
        return True
    else:
        print("‚ùå –ù–µ—É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏—è—Ç–∞")
        return False


if __name__ == "__main__":
    print("=== –ü–û–î–û–ë–†–ï–ù COINDESK SCRAPER TEST ===")
    success = test_single_article()

    if success:
        print("\nüéâ Single article test —É—Å–ø–µ—à–µ–Ω!")
    else:
        print("‚ùå Single article test –Ω–µ—É—Å–ø–µ—à–µ–Ω")
