import sqlite3
import json
from datetime import datetime
from pathlib import Path


class DatabaseManager:
    def __init__(self, db_path="crypto_news.db"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞ database connection"""
        self.db_path = db_path
        print(f"üóÑÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–Ω–∏: {db_path}")
        self.init_database()
        print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω–∏ –≥–æ—Ç–æ–≤–∞!")

    def init_database(self):
        """–°—ä–∑–¥–∞–≤–∞ —Ç–∞–±–ª–∏—Ü–∏—Ç–µ –∞–∫–æ –Ω–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—Ç"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # –ì–ª–∞–≤–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ –∑–∞ —Å—Ç–∞—Ç–∏–∏
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS articles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    url TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    content TEXT NOT NULL,
                    author TEXT,
                    published_date TEXT,
                    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    content_length INTEGER,
                    processed BOOLEAN DEFAULT FALSE,
                    analyzed_at TIMESTAMP NULL,
                    sentiment_result TEXT NULL
                )
            ''')

            # –¢–∞–±–ª–∏—Ü–∞ –∑–∞ scraped URLs (history)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS scraped_urls (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    url TEXT UNIQUE NOT NULL,
                    first_scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_seen_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    scrape_count INTEGER DEFAULT 1
                )
            ''')

            # –ò–Ω–¥–µ–∫—Å–∏ –∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–Ω–æ—Å—Ç
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_url ON articles(url)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_processed ON articles(processed)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_scraped_urls_url ON scraped_urls(url)')

            conn.commit()

    def is_article_exists(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ —Å—Ç–∞—Ç–∏—è—Ç–∞ –≤–µ—á–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞ –≤ –±–∞–∑–∞—Ç–∞"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM articles WHERE url = ?", (url,))
            return cursor.fetchone() is not None

    def is_url_scraped_before(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ URL-–∞ –µ –±–∏–ª —Å–∫—Ä–∞–ø–≤–∞–Ω –ø—Ä–µ–¥–∏"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM scraped_urls WHERE url = ?", (url,))
            return cursor.fetchone() is not None

    def record_scraped_url(self, url):
        """–ó–∞–ø–∏—Å–≤–∞ –∏–ª–∏ update-–≤–∞ URL –≤ –∏—Å—Ç–æ—Ä–∏—è—Ç–∞"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            if self.is_url_scraped_before(url):
                # Update existing record
                cursor.execute('''
                    UPDATE scraped_urls 
                    SET last_seen_at = CURRENT_TIMESTAMP, 
                        scrape_count = scrape_count + 1 
                    WHERE url = ?
                ''', (url,))
            else:
                # Insert new record
                cursor.execute('''
                    INSERT INTO scraped_urls (url) VALUES (?)
                ''', (url,))

            conn.commit()

    def save_article(self, article_data):
        """–ó–∞–ø–∞–∑–≤–∞ —Å—Ç–∞—Ç–∏—è –≤ –±–∞–∑–∞—Ç–∞ –¥–∞–Ω–Ω–∏"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # –ó–∞–ø–∏—Å–≤–∞–º–µ —Å—Ç–∞—Ç–∏—è—Ç–∞
                cursor.execute('''
                    INSERT OR IGNORE INTO articles 
                    (url, title, content, author, published_date, content_length)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    article_data['url'],
                    article_data['title'],
                    article_data['content'],
                    article_data['author'],
                    str(article_data['date']),
                    article_data['content_length']
                ))

                # –ó–∞–ø–∏—Å–≤–∞–º–µ –≤ URL –∏—Å—Ç–æ—Ä–∏—è—Ç–∞
                self.record_scraped_url(article_data['url'])

                conn.commit()

                if cursor.rowcount > 0:
                    print(f"‚úÖ –ó–∞–ø–∞–∑–µ–Ω–∞ —Å—Ç–∞—Ç–∏—è: {article_data['title'][:50]}...")
                    return True
                else:
                    print(f"‚ö†Ô∏è –°—Ç–∞—Ç–∏—è—Ç–∞ –≤–µ—á–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞: {article_data['title'][:50]}...")
                    return False

        except Exception as e:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏—è: {str(e)}")
            return False

    def save_multiple_articles(self, articles):
        """–ó–∞–ø–∞–∑–≤–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–∏–∏"""
        print(f"üíæ –ó–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ {len(articles)} —Å—Ç–∞—Ç–∏–∏ –≤ –±–∞–∑–∞—Ç–∞...")

        saved_count = 0
        duplicate_count = 0

        for article in articles:
            if self.save_article(article):
                saved_count += 1
            else:
                duplicate_count += 1

        print(f"üìä –†–µ–∑—É–ª—Ç–∞—Ç: {saved_count} –Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏, {duplicate_count} –¥—É–±–ª–∏—Ä–∞—â–∏ —Å–µ")
        return saved_count, duplicate_count

    def get_unprocessed_articles(self, limit=None):
        """–í—Ä—ä—â–∞ –Ω–µ–ø—Ä–æ—Ü–µ—Å–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏ –∑–∞ –∞–Ω–∞–ª–∏–∑"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row  # –ó–∞ dictionary-like —Ä–µ–∑—É–ª—Ç–∞—Ç–∏
            cursor = conn.cursor()

            query = '''
                SELECT id, url, title, content, author, published_date, content_length, scraped_at
                FROM articles 
                WHERE processed = FALSE 
                ORDER BY scraped_at DESC
            '''

            if limit:
                query += f" LIMIT {limit}"

            cursor.execute(query)
            return [dict(row) for row in cursor.fetchall()]

    def mark_article_as_analyzed(self, article_id, sentiment_result=None):
        """–ú–∞—Ä–∫–∏—Ä–∞ —Å—Ç–∞—Ç–∏—è –∫–∞—Ç–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∞"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            sentiment_json = json.dumps(sentiment_result) if sentiment_result else None

            cursor.execute('''
                UPDATE articles 
                SET processed = TRUE, 
                    analyzed_at = CURRENT_TIMESTAMP,
                    sentiment_result = ?
                WHERE id = ?
            ''', (sentiment_json, article_id))

            conn.commit()
            print(f"‚úÖ –°—Ç–∞—Ç–∏—è {article_id} –º–∞—Ä–∫–∏—Ä–∞–Ω–∞ –∫–∞—Ç–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∞")

    def cleanup_old_analyzed_articles(self, days_to_keep=7):
        """–ò–∑—Ç—Ä–∏–≤–∞ —Å—Ç–∞—Ä–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏ (scraped_urls –æ—Å—Ç–∞–≤–∞—Ç!)"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            cursor.execute('''
                DELETE FROM articles 
                WHERE processed = TRUE 
                AND analyzed_at IS NOT NULL 
                AND datetime(analyzed_at) < datetime('now', '-{} days')
            '''.format(days_to_keep))

            deleted_count = cursor.rowcount
            conn.commit()

            print(f"üßπ –ò–∑—Ç—Ä–∏—Ç–∏ {deleted_count} —Å—Ç–∞—Ä–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏")
            return deleted_count

    def get_database_stats(self):
        """–í—Ä—ä—â–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞ –±–∞–∑–∞—Ç–∞ –¥–∞–Ω–Ω–∏"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # –û–±—â –±—Ä–æ–π —Å—Ç–∞—Ç–∏–∏
            cursor.execute("SELECT COUNT(*) FROM articles")
            total_articles = cursor.fetchone()[0]

            # –ù–µ–ø—Ä–æ—Ü–µ—Å–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏
            cursor.execute("SELECT COUNT(*) FROM articles WHERE processed = FALSE")
            unprocessed_articles = cursor.fetchone()[0]

            # –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏
            cursor.execute("SELECT COUNT(*) FROM articles WHERE processed = TRUE")
            analyzed_articles = cursor.fetchone()[0]

            # –û–±—â–æ scraped URLs
            cursor.execute("SELECT COUNT(*) FROM scraped_urls")
            total_scraped_urls = cursor.fetchone()[0]

            # –ù–∞–π-–Ω–æ–≤–∞ —Å—Ç–∞—Ç–∏—è
            cursor.execute("SELECT title, scraped_at FROM articles ORDER BY scraped_at DESC LIMIT 1")
            latest_article = cursor.fetchone()

            return {
                'total_articles': total_articles,
                'unprocessed_articles': unprocessed_articles,
                'analyzed_articles': analyzed_articles,
                'total_scraped_urls': total_scraped_urls,
                'latest_article': latest_article
            }

    def export_articles_to_json(self, filename="articles_export.json", processed_only=False):
        """–ï–∫—Å–ø–æ—Ä—Ç–∏—Ä–∞ —Å—Ç–∞—Ç–∏–∏ –≤ JSON —Ñ–∞–π–ª"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            query = "SELECT * FROM articles"
            if processed_only:
                query += " WHERE processed = TRUE"
            query += " ORDER BY scraped_at DESC"

            cursor.execute(query)
            articles = [dict(row) for row in cursor.fetchall()]

            # –ó–∞–ø–∞–∑–≤–∞–º–µ –≤ JSON —Ñ–∞–π–ª
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2, default=str)

            print(f"üì§ –ï–∫—Å–ø–æ—Ä—Ç–∏—Ä–∞–Ω–∏ {len(articles)} —Å—Ç–∞—Ç–∏–∏ –≤ {filename}")
            return len(articles)


# –¢–µ—Å—Ç–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏—è
def test_database():
    """–¢–µ—Å—Ç–≤–∞ database —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç—Ç–∞"""
    print("=== –¢–ï–°–¢ –ù–ê DATABASE ===")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–º–µ database
    db = DatabaseManager("test_crypto_news.db")

    # –¢–µ—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏
    test_article = {
        'url': 'https://example.com/test-article',
        'title': 'Test Bitcoin Article',
        'content': 'This is a test article about Bitcoin and cryptocurrency market trends.',
        'author': 'Test Author',
        'date': '2025-06-09',
        'content_length': 75
    }

    # –¢–µ—Å—Ç–≤–∞–º–µ –∑–∞–ø–∞–∑–≤–∞–Ω–µ
    print("\n1. –¢–µ—Å—Ç–≤–∞–Ω–µ –Ω–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏—è...")
    success = db.save_article(test_article)
    print(f"–†–µ–∑—É–ª—Ç–∞—Ç: {'–£—Å–ø–µ—Ö' if success else '–ù–µ—É—Å–ø–µ—Ö'}")

    # –¢–µ—Å—Ç–≤–∞–º–µ –¥—É–±–ª–∏—Ä–∞–Ω–æ –∑–∞–ø–∞–∑–≤–∞–Ω–µ
    print("\n2. –¢–µ—Å—Ç–≤–∞–Ω–µ –Ω–∞ –¥—É–±–ª–∏—Ä–∞–Ω–æ –∑–∞–ø–∞–∑–≤–∞–Ω–µ...")
    success = db.save_article(test_article)
    print(f"–†–µ–∑—É–ª—Ç–∞—Ç: {'–ù–µ–æ—á–∞–∫–≤–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ' if success else '–ü—Ä–∞–≤–∏–ª–Ω–æ –±–ª–æ–∫–∏—Ä–∞–Ω–æ –¥—É–±–ª–∏—Ä–∞–Ω–µ'}")

    # –¢–µ—Å—Ç–≤–∞–º–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print("\n3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–∞ –±–∞–∑–∞—Ç–∞ –¥–∞–Ω–Ω–∏:")
    stats = db.get_database_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")

    # –¢–µ—Å—Ç–≤–∞–º–µ unprocessed —Å—Ç–∞—Ç–∏–∏
    print("\n4. –ù–µ–ø—Ä–æ—Ü–µ—Å–∏—Ä–∞–Ω–∏ —Å—Ç–∞—Ç–∏–∏:")
    unprocessed = db.get_unprocessed_articles()
    print(f"   –ù–∞–º–µ—Ä–µ–Ω–∏: {len(unprocessed)} —Å—Ç–∞—Ç–∏–∏")

    # –ü–æ—á–∏—Å—Ç–≤–∞–º–µ test –±–∞–∑–∞—Ç–∞
    Path("test_crypto_news.db").unlink(missing_ok=True)
    print("\n‚úÖ Database —Ç–µ—Å—Ç –∑–∞–≤—ä—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")


if __name__ == "__main__":
    test_database()
