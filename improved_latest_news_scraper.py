import sqlite3

import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime, timedelta
from urllib.parse import urljoin
import re
import json

from config import (
    COINDESK_BASE_URL,
    REQUEST_HEADERS,
    SCRAPING_CONFIG,
    HTML_SELECTORS
)
from database import DatabaseManager


class CoinDeskLatestNewsScraper:
    def __init__(self, use_database=True):
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ CoinDesk Latest News Scraper...")
        self.session = requests.Session()

        # Headers
        simple_headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
        }
        self.session.headers.update(simple_headers)

        # URL –∑–∞ latest news
        self.latest_news_url = "https://www.coindesk.com/latest-crypto-news"

        # Database –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
        self.use_database = use_database
        if use_database:
            self.db = DatabaseManager()
        else:
            self.db = None

        print("‚úÖ Latest News Scraper –≥–æ—Ç–æ–≤!")

    def get_articles_by_date_filter(self, date_filter='today', max_articles=50):
        """
        –ü–æ–ª—É—á–∞–≤–∞ —Å—Ç–∞—Ç–∏–∏ –æ—Ç latest-crypto-news —Å date —Ñ–∏–ª—Ç—ä—Ä

        date_filter –º–æ–∂–µ –¥–∞ –µ:
        - 'today' - —Å–∞–º–æ –¥–Ω–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏
        - 'yesterday' - –≤—á–µ—Ä–∞—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏
        - '2025-06-10' - –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –¥–∞—Ç–∞
        - 'last_3_days' - –ø–æ—Å–ª–µ–¥–Ω–∏—Ç–µ 3 –¥–Ω–∏
        - 'all' - –≤—Å–∏—á–∫–∏ (–¥–æ max_articles)
        """
        print(f"üîç –¢—ä—Ä—Å–µ–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏–∏ —Å —Ñ–∏–ª—Ç—ä—Ä: {date_filter}")

        # –û–ø—Ä–µ–¥–µ–ª—è–º–µ target –¥–∞—Ç–∏
        target_dates = self._get_target_dates(date_filter)
        print(f"üìÖ Target –¥–∞—Ç–∏: {target_dates}")

        # –ó–∞–ø–æ—á–≤–∞–º–µ –¥–∞ —Å–∫—Ä–∞–ø–≤–∞–º–µ —Å—Ç—Ä–∞–Ω–∏—Ü–∏
        all_articles = []
        page_offset = 0
        pages_checked = 0
        max_pages = 10  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç

        while len(all_articles) < max_articles and pages_checked < max_pages:
            print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {pages_checked + 1}...")

            # –°–∫—Ä–∞–ø–≤–∞–º–µ —Ç–µ–∫—É—â–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞
            page_articles = self._scrape_latest_news_page(page_offset)

            if not page_articles:
                print("‚ùå –ù—è–º–∞ –ø–æ–≤–µ—á–µ —Å—Ç–∞—Ç–∏–∏")
                break

            # –§–∏–ª—Ç—Ä–∏—Ä–∞–º–µ –ø–æ –¥–∞—Ç–∞
            filtered_articles = []
            for article in page_articles:
                article_date = self._extract_date_from_article_data(article)
                if article_date in target_dates or date_filter == 'all':
                    filtered_articles.append(article)
                elif date_filter != 'all' and article_date < min(target_dates):
                    # –ê–∫–æ —Å—Ç–∞—Ç–∏—è—Ç–∞ –µ –ø–æ-—Å—Ç–∞—Ä–∞ –æ—Ç –Ω–∞–π-—Å—Ç–∞—Ä–∞—Ç–∞ target –¥–∞—Ç–∞, —Å–ø–∏—Ä–∞–º–µ
                    print(f"‚èπÔ∏è –î–æ—Å—Ç–∏–≥–Ω–∞—Ö–º–µ —Å—Ç–∞—Ä–∏ —Å—Ç–∞—Ç–∏–∏ ({article_date}), —Å–ø–∏—Ä–∞–º–µ")
                    return all_articles[:max_articles]

            all_articles.extend(filtered_articles)
            pages_checked += 1
            page_offset += 16  # CoinDesk –ø–æ–∫–∞–∑–≤–∞ 16 —Å—Ç–∞—Ç–∏–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞

            print(f"üìä –°—Ç—Ä–∞–Ω–∏—Ü–∞ {pages_checked}: {len(filtered_articles)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏ —Å—Ç–∞—Ç–∏–∏")

            # –ú–∞–ª–∫–∞ –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∏
            time.sleep(2)

        print(f"‚úÖ –ù–∞–º–µ—Ä–µ–Ω–∏ {len(all_articles)} —Å—Ç–∞—Ç–∏–∏ —Å —Ñ–∏–ª—Ç—ä—Ä '{date_filter}'")
        return all_articles[:max_articles]

    def _get_target_dates(self, date_filter):
        """–í—Ä—ä—â–∞ —Å–ø–∏—Å—ä–∫ —Å target –¥–∞—Ç–∏ –∑–∞ —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ"""
        today = datetime.now().date()

        if date_filter == 'today':
            return [today]
        elif date_filter == 'yesterday':
            yesterday = today - timedelta(days=1)
            return [yesterday]
        elif date_filter == 'last_3_days':
            return [today - timedelta(days=i) for i in range(3)]
        elif date_filter == 'last_week':
            return [today - timedelta(days=i) for i in range(7)]
        elif isinstance(date_filter, str) and re.match(r'\d{4}-\d{2}-\d{2}', date_filter):
            # –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç YYYY-MM-DD
            target_date = datetime.strptime(date_filter, '%Y-%m-%d').date()
            return [target_date]
        else:
            # 'all' –∏–ª–∏ –Ω–µ—Ä–∞–∑–ø–æ–∑–Ω–∞—Ç —Ñ–∏–ª—Ç—ä—Ä
            return []

    def _scrape_latest_news_page(self, offset=0):
        """–°–∫—Ä–∞–ø–≤–∞ –µ–¥–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ—Ç latest-crypto-news"""
        try:
            # URL –∑–∞ pagination –º–æ–∂–µ –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞ offset –ø–∞—Ä–∞–º–µ—Ç—ä—Ä
            url = f"{self.latest_news_url}?offset={offset}" if offset > 0 else self.latest_news_url

            response = self.session.get(url, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # –¢—ä—Ä—Å–∏–º —Å—Ç–∞—Ç–∏–∏ - –æ–±–∏–∫–Ω–æ–≤–µ–Ω–æ —Å–∞ –≤ article elements –∏–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏ containers
            articles = []

            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –¢—ä—Ä—Å–∏–º article elements
            article_elements = soup.find_all('article')
            print(f"üîç –ù–∞–º–µ—Ä–µ–Ω–∏ {len(article_elements)} article elements")

            for article_elem in article_elements:
                article_data = self._extract_article_data_from_element(article_elem)
                if article_data:
                    articles.append(article_data)

            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ê–∫–æ –Ω—è–º–∞ article elements, —Ç—ä—Ä—Å–∏–º –ø–æ –ª–∏–Ω–∫–æ–≤–µ
            if not articles:
                print("üîç –¢—ä—Ä—Å—è —Å—Ç–∞—Ç–∏–∏ –ø–æ –ª–∏–Ω–∫–æ–≤–µ...")
                link_elements = soup.find_all('a', href=True)
                for link in link_elements:
                    href = link['href']
                    if self._is_valid_article_url(href):
                        article_data = {
                            'url': self._make_full_url(href),
                            'title': link.get_text().strip(),
                            'href': href
                        }
                        if article_data['title'] and len(article_data['title']) > 15:
                            articles.append(article_data)

            return articles[:16]  # CoinDesk –ø–æ–∫–∞–∑–≤–∞ 16 –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞

        except Exception as e:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ scraping –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {e}")
            return []

    def _extract_article_data_from_element(self, article_elem):
        """–ò–∑–≤–ª–∏—á–∞ –¥–∞–Ω–Ω–∏ –∑–∞ —Å—Ç–∞—Ç–∏—è –æ—Ç article element"""
        try:
            # –¢—ä—Ä—Å–∏–º –ª–∏–Ω–∫ –≤ article element
            link = article_elem.find('a', href=True)
            if not link:
                return None

            href = link['href']
            if not self._is_valid_article_url(href):
                return None

            # –ò–∑–≤–ª–∏—á–∞–º–µ –∑–∞–≥–ª–∞–≤–∏–µ—Ç–æ
            title = ""
            # –¢—ä—Ä—Å–∏–º h1, h2, h3 –≤ article
            for header_tag in ['h1', 'h2', 'h3']:
                header = article_elem.find(header_tag)
                if header:
                    title = header.get_text().strip()
                    break

            # –ê–∫–æ –Ω—è–º–∞ header, –≤–∑–µ–º–∞–º–µ –æ—Ç link text
            if not title:
                title = link.get_text().strip()

            if not title or len(title) < 15:
                return None

            return {
                'url': self._make_full_url(href),
                'title': title,
                'href': href
            }

        except Exception as e:
            print(f"‚ö†Ô∏è –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ article data: {e}")
            return None

    def _is_valid_article_url(self, href):
        """–ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ URL –µ –≤–∞–ª–∏–¥–µ–Ω –∑–∞ —Å—Ç–∞—Ç–∏—è"""
        if not href or href in ['#', '/', '']:
            return False

        if href.startswith('http') and 'coindesk.com' not in href:
            return False

        if href.startswith('#') or href.startswith('mailto:') or href.startswith('tel:'):
            return False

        # –ò–∑–∫–ª—é—á–≤–∞–º–µ —Å–∏—Å—Ç–µ–º–Ω–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∏
        exclude_patterns = [
            '/newsletters/', '/podcasts/', '/events/', '/about/', '/careers/',
            '/advertise/', '/price/', '/author/', '/tag/', '/sponsored-content/',
            '/_next/', '/api/', '/search', '/privacy', '/terms'
        ]

        for pattern in exclude_patterns:
            if pattern in href:
                return False

        # –ü—Ä–∏–µ–º–∞–º–µ —Å—Ç–∞—Ç–∏–∏ —Å –¥–∞—Ç–∞ –∏–ª–∏ –æ—Ç news –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        date_patterns = [
            r'/\d{4}/\d{2}/\d{2}/',  # /2025/06/10/
            r'/2025/', r'/2024/'
        ]

        has_date = any(re.search(pattern, href) for pattern in date_patterns)

        news_categories = ['/markets/', '/policy/', '/tech/', '/business/', '/layer2/', '/web3/', '/daybook']
        has_category = any(category in href for category in news_categories)

        return has_date or has_category

    def _make_full_url(self, href):
        """–ü—Ä–∞–≤–∏ –ø—ä–ª–µ–Ω URL –æ—Ç relative href"""
        if href.startswith('http'):
            return href
        elif href.startswith('/'):
            return f"https://www.coindesk.com{href}"
        else:
            return f"https://www.coindesk.com/{href}"

    def _extract_date_from_article_data(self, article_data):
        """–ò–∑–≤–ª–∏—á–∞ –¥–∞—Ç–∞ –æ—Ç article data"""
        url = article_data['url']

        # –û–ø–∏—Ç–≤–∞–º–µ –¥–∞ –∏–∑–≤–ª–µ—á–µ–º –æ—Ç URL
        date_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', url)
        if date_match:
            year, month, day = date_match.groups()
            try:
                return datetime(int(year), int(month), int(day)).date()
            except:
                pass

        # –ê–∫–æ –Ω—è–º–∞ –¥–∞—Ç–∞ –≤ URL, –ø—Ä–∏–µ–º–∞–º–µ —á–µ –µ –æ—Ç –¥–Ω–µ—Å–∫–∞ (latest news)
        return datetime.now().date()

    def scrape_articles_smart(self, date_filter='today', limit=10, save_to_db=True):
        """
        Smart scraping —Å date —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ

        date_filter:
        - 'today' - —Å–∞–º–æ –¥–Ω–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏
        - 'yesterday' - –≤—á–µ—Ä–∞—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏
        - '2025-06-10' - –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –¥–∞—Ç–∞
        - 'last_3_days' - –ø–æ—Å–ª–µ–¥–Ω–∏—Ç–µ 3 –¥–Ω–∏
        """
        print(f"üéØ Smart scraping: {limit} —Å—Ç–∞—Ç–∏–∏ —Å —Ñ–∏–ª—Ç—ä—Ä '{date_filter}'")

        # –í–∑–µ–º–∞–º–µ —Å—Ç–∞—Ç–∏–∏—Ç–µ —Å —Ñ–∏–ª—Ç—ä—Ä
        article_links = self.get_articles_by_date_filter(date_filter, max_articles=limit * 2)

        if not article_links:
            print("‚ùå –ù–µ —Å–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —Å—Ç–∞—Ç–∏–∏ —Å —Ç–æ–∑–∏ —Ñ–∏–ª—Ç—ä—Ä")
            return []

        # Database —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ
        if self.db and save_to_db:
            print("üîç –ü—Ä–æ–≤–µ—Ä—è–≤–∞–Ω–µ –∑–∞ –¥—É–±–ª–∏—Ä–∞—â–∏ —Å–µ URLs...")
            new_article_links = []
            skipped_count = 0

            for link_info in article_links:
                url = link_info['url']
                if not self.db.is_url_scraped_before(url):
                    new_article_links.append(link_info)
                else:
                    skipped_count += 1
                    self.db.record_scraped_url(url)

            print(f"üìä {len(new_article_links)} –Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏, {skipped_count} –≤–µ—á–µ scraped")
            article_links = new_article_links

        # –û–≥—Ä–∞–Ω–∏—á–∞–≤–∞–º–µ –¥–æ –ª–∏–º–∏—Ç–∞
        article_links = article_links[:limit]

        if not article_links:
            print("‚ÑπÔ∏è –í—Å–∏—á–∫–∏ —Å—Ç–∞—Ç–∏–∏ —Å–∞ –≤–µ—á–µ scraped")
            return []

        # Scraping –Ω–∞ —Å—Ç–∞—Ç–∏–∏—Ç–µ
        scraped_articles = []
        successful_count = 0
        failed_count = 0

        for i, link_info in enumerate(article_links, 1):
            url = link_info['url']
            print(f"\n[{i}/{len(article_links)}] {link_info['title'][:60]}...")

            article_data = self.scrape_single_article(url)
            if article_data:
                scraped_articles.append(article_data)
                successful_count += 1

                if self.db and save_to_db:
                    self.db.save_article(article_data)
            else:
                failed_count += 1

        print(f"\nüéâ Smart scraping –∑–∞–≤—ä—Ä—à–µ–Ω!")
        print(f"üìä –†–µ–∑—É–ª—Ç–∞—Ç: {successful_count} —É—Å–ø–µ—à–Ω–∏, {failed_count} –Ω–µ—É—Å–ø–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏")

        return scraped_articles

    def scrape_single_article(self, article_url):
        """–ò–∑–≤–ª–∏—á–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–∞ –µ–¥–Ω–∞ —Å—Ç–∞—Ç–∏—è (–∏–∑–ø–æ–ª–∑–≤–∞ —Å—ä—â–∞—Ç–∞ –ª–æ–≥–∏–∫–∞ –∫–∞—Ç–æ —Å—Ç–∞—Ä–∏—è scraper)"""
        print(f"üìÑ Scraping —Å—Ç–∞—Ç–∏—è: {article_url}")

        try:
            time.sleep(SCRAPING_CONFIG['delay_between_requests'])

            response = self.session.get(article_url, timeout=15)
            response.raise_for_status()

            content_text = response.content.decode('utf-8', errors='ignore')
            soup = BeautifulSoup(content_text, 'html.parser')

            # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ —Å—ä—â–∏—Ç–µ –º–µ—Ç–æ–¥–∏ –∑–∞ –∏–∑–≤–ª–∏—á–∞–Ω–µ –∫–∞—Ç–æ –≤ —Å—Ç–∞—Ä–∏—è scraper
            title = self._extract_title_improved(soup)
            content = self._extract_content_improved(soup)
            date = self._extract_date_improved(soup)
            author = self._extract_author_improved(soup)

            if len(content) < SCRAPING_CONFIG['min_article_length']:
                print(f"‚ö†Ô∏è  –°—Ç–∞—Ç–∏—è—Ç–∞ –µ —Ç–≤—ä—Ä–¥–µ –∫—Ä–∞—Ç–∫–∞ ({len(content)} chars)")
                return None

            article_data = {
                'url': article_url,
                'title': title,
                'content': content,
                'date': date,
                'author': author,
                'scraped_at': datetime.now(),
                'content_length': len(content)
            }

            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—è: {title[:50]}... ({len(content)} chars)")
            return article_data

        except Exception as e:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ scraping –Ω–∞ {article_url}: {str(e)}")
            return None

    # –°—ä—â–∏—Ç–µ –º–µ—Ç–æ–¥–∏ –∑–∞ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ –∫–∞—Ç–æ –≤ —Å—Ç–∞—Ä–∏—è scraper
    def _extract_title_improved(self, soup):
        h1_tags = soup.find_all('h1')
        for h1 in h1_tags:
            text = h1.get_text().strip()
            if text and len(text) > 10:
                return text

        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title['content'].strip()

        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
            title = re.sub(r'\s*\|\s*CoinDesk.*$', '', title)
            if title:
                return title
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –∑–∞–≥–ª–∞–≤–∏–µ"

    def _extract_content_improved(self, soup):
        print("üîç –ó–∞–ø–æ—á–≤–∞–º –ø–æ–¥–æ–±—Ä–µ–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ...")

        main_selectors = ['main', 'article', '[role="main"]', '.article-content', '.post-content']

        for selector in main_selectors:
            container = soup.select_one(selector)
            if container:
                paragraphs = container.find_all('p')
                content = self._process_paragraphs(paragraphs)
                if len(content) > 200:
                    return content

        all_paragraphs = soup.find_all('p')
        if all_paragraphs:
            content = self._process_paragraphs(all_paragraphs)
            if len(content) > 100:
                return content

        return "–°—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–µ –º–æ–∂–µ –¥–∞ –±—ä–¥–µ –∏–∑–≤–ª–µ—á–µ–Ω–æ"

    def _process_paragraphs(self, paragraphs):
        meaningful_paragraphs = []
        for p in paragraphs:
            text = p.get_text().strip()
            if self._is_meaningful_paragraph(text):
                meaningful_paragraphs.append(text)
        return '\n\n'.join(meaningful_paragraphs)

    def _is_meaningful_paragraph(self, text):
        if len(text) < 20:
            return False
        exclude_phrases = [
            'Sign up', 'Subscribe', 'Newsletter', 'See all newsletters',
            'Don\'t miss', 'privacy policy', 'terms of use', 'Cookie'
        ]
        text_lower = text.lower()
        for phrase in exclude_phrases:
            if phrase.lower() in text_lower:
                return False
        return text.count('.') >= 1

    def _extract_date_improved(self, soup):
        published_meta = soup.find('meta', property='article:published_time')
        if published_meta and published_meta.get('content'):
            try:
                date_str = published_meta['content']
                return datetime.fromisoformat(date_str.replace('Z', '+00:00')).strftime('%Y-%m-%d')
            except:
                pass
        return datetime.now().strftime('%Y-%m-%d')

    def _extract_author_improved(self, soup):
        author_meta = soup.find('meta', attrs={'name': 'author'})
        if author_meta and author_meta.get('content'):
            return author_meta['content'].strip()
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω –∞–≤—Ç–æ—Ä"

    def get_scraping_status_by_date(self, date_str):
        """–ü–æ–∫–∞–∑–≤–∞ —Å—Ç–∞—Ç—É—Å –Ω–∞ scraping –∑–∞ –¥–∞–¥–µ–Ω–∞ –¥–∞—Ç–∞"""
        if not self.db:
            return {'error': 'Database –Ω–µ –µ –∞–∫—Ç–∏–≤–Ω–∞'}

        try:
            conn = self.db.get_connection() if hasattr(self.db, 'get_connection') else sqlite3.connect(self.db.db_path)
            cursor = conn.cursor()

            # –°—Ç–∞—Ç–∏–∏ –æ—Ç —Ç–∞–∑–∏ –¥–∞—Ç–∞
            cursor.execute("""
                SELECT COUNT(*) FROM articles 
                WHERE url LIKE ? OR published_date = ?
            """, (f'%{date_str.replace("-", "/")}%', date_str))

            scraped_count = cursor.fetchone()[0]

            # –ù–∞–º–∏—Ä–∞–º–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–Ω–∏ —Å—Ç–∞—Ç–∏–∏ –æ—Ç latest news
            potential_articles = self.get_articles_by_date_filter(date_str, max_articles=50)
            potential_count = len(potential_articles)

            new_count = 0
            for article in potential_articles:
                cursor.execute("SELECT 1 FROM scraped_urls WHERE url = ?", (article['url'],))
                if not cursor.fetchone():
                    new_count += 1

            conn.close()

            return {
                'date': date_str,
                'scraped_articles': scraped_count,
                'potential_articles': potential_count,
                'new_to_scrape': new_count,
                'completion_rate': f"{(scraped_count / potential_count * 100):.1f}%" if potential_count > 0 else "N/A"
            }

        except Exception as e:
            return {'error': str(e)}


# –¢–µ—Å—Ç–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏
def test_latest_news_scraper():
    """–¢–µ—Å—Ç–≤–∞ –Ω–æ–≤–∏—è latest news scraper"""
    print("=== –¢–ï–°–¢ –ù–ê LATEST NEWS SCRAPER ===")

    scraper = CoinDeskLatestNewsScraper(use_database=False)

    # –¢–µ—Å—Ç 1: –î–Ω–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏
    print("\n1. –¢–µ—Å—Ç: –î–Ω–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏")
    today_articles = scraper.get_articles_by_date_filter('today', max_articles=10)
    print(f"üìä –ù–∞–º–µ—Ä–µ–Ω–∏ {len(today_articles)} –¥–Ω–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏")

    for i, article in enumerate(today_articles[:3], 1):
        print(f"   {i}. {article['title'][:60]}...")

    # –¢–µ—Å—Ç 2: Smart scraping
    print("\n2. –¢–µ—Å—Ç: Smart scraping –Ω–∞ 3 —Å—Ç–∞—Ç–∏–∏")
    scraped = scraper.scrape_articles_smart('today', limit=3, save_to_db=False)
    print(f"üìä –£—Å–ø–µ—à–Ω–æ scraped: {len(scraped)} —Å—Ç–∞—Ç–∏–∏")

    return len(scraped) > 0


if __name__ == "__main__":
    success = test_latest_news_scraper()
    if success:
        print("\n‚úÖ Latest News Scraper —Ä–∞–±–æ—Ç–∏ –æ—Ç–ª–∏—á–Ω–æ!")
    else:
        print("\n‚ùå –ò–º–∞ –ø—Ä–æ–±–ª–µ–º–∏ —Å Latest News Scraper")
