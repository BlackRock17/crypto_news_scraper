import sqlite3

import requests


def _extract_content_improved(self, soup):
    """–ö–û–†–ï–ù–ù–û –ü–û–î–û–ë–†–ï–ù–û –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –∑–∞ CoinDesk"""

    print("üîç –ó–∞–ø–æ—á–≤–∞–º FIXED content extraction...")

    # –°–¢–†–ê–¢–ï–ì–ò–Ø 1: CoinDesk-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏ patterns
    print("üéØ –°–¢–†–ê–¢–ï–ì–ò–Ø 1: CoinDesk patterns...")

    # –ù–∞–º–∏—Ä–∞–º–µ "What to know:" marker –∏ –≤–∑–∏–º–∞–º–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
    what_to_know = soup.find(text=lambda text: text and 'What to know:' in text)
    if what_to_know:
        print("‚úÖ –ù–∞–º–µ—Ä–µ–Ω 'What to know:' marker")

        # –ù–∞–º–∏—Ä–∞–º–µ parent container
        current = what_to_know.parent
        while current and current.name not in ['main', 'article', 'div', 'section']:
            current = current.parent

        if current:
            # –í–∑–µ–º–∞–º–µ –≤—Å–∏—á–∫–∏ <p> –≤ —Ç–æ–∑–∏ container
            container_paragraphs = current.find_all('p')
            meaningful_text = []

            for p in container_paragraphs:
                text = p.get_text().strip()
                if (text and
                        len(text) > 30 and
                        'See all newsletters' not in text and
                        'What to know:' not in text and
                        not text.startswith('[')):  # –ü—Ä–µ–º–∞—Ö–≤–∞–º–µ price links
                    meaningful_text.append(text)

            if meaningful_text:
                content = '\n\n'.join(meaningful_text)
                print(f"‚úÖ CoinDesk pattern extraction: {len(content)} chars")
                if len(content) > 200:
                    return content

    # –°–¢–†–ê–¢–ï–ì–ò–Ø 2: –¢—ä—Ä—Å–∏–º main content container
    print("üéØ –°–¢–†–ê–¢–ï–ì–ò–Ø 2: Main containers...")
    main_selectors = [
        'main',
        'article',
        '[role="main"]',
        '.article-content',
        '.post-content',
        '.entry-content',
        'div[data-module="ArticleBody"]'
    ]

    for selector in main_selectors:
        container = soup.select_one(selector)
        if container:
            print(f"‚úÖ –ù–∞–º–µ—Ä–µ–Ω container: {selector}")
            paragraphs = container.find_all('p')
            content = self._process_paragraphs_fixed(paragraphs)
            if len(content) > 200:
                print(f"‚úÖ Main container extraction: {len(content)} chars")
                return content

    # –°–¢–†–ê–¢–ï–ì–ò–Ø 3: –í—Å–∏—á–∫–∏ <p> tags —Å –ø–æ-—É–º–Ω–∞ —Ñ–∏–ª—Ç—Ä–∞—Ü–∏—è
    print("üéØ –°–¢–†–ê–¢–ï–ì–ò–Ø 3: –í—Å–∏—á–∫–∏ <p> tags...")
    all_paragraphs = soup.find_all('p')
    print(f"üìä –ù–∞–º–µ—Ä–µ–Ω–∏ {len(all_paragraphs)} –æ–±—â–æ <p> tags")

    if all_paragraphs:
        content = self._process_paragraphs_fixed(all_paragraphs)
        if len(content) > 100:
            print(f"‚úÖ All paragraphs extraction: {len(content)} chars")
            return content

    # –°–¢–†–ê–¢–ï–ì–ò–Ø 4: –¢—ä—Ä—Å–∏–º —Ç–µ–∫—Å—Ç –≤ div –µ–ª–µ–º–µ–Ω—Ç–∏
    print("üéØ –°–¢–†–ê–¢–ï–ì–ò–Ø 4: Div text extraction...")

    # –ù–∞–º–∏—Ä–∞–º–µ –≤—Å–∏—á–∫–∏ div-–æ–≤–µ —Å —Ç–µ–∫—Å—Ç
    text_divs = soup.find_all('div')
    meaningful_texts = []

    for div in text_divs:
        # –í–∑–∏–º–∞–º–µ –¥–∏—Ä–µ–∫—Ç–Ω–∏—è —Ç–µ–∫—Å—Ç –æ—Ç div-–∞
        direct_text = div.get_text(separator=' ', strip=True)

        # –§–∏–ª—Ç—Ä–∏—Ä–∞–º–µ –ø–æ –¥—ä–ª–∂–∏–Ω–∞ –∏ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ
        if (50 < len(direct_text) < 2000 and
                '.' in direct_text and  # –¢—Ä—è–±–≤–∞ –¥–∞ –∏–º–∞ –∏–∑—Ä–µ—á–µ–Ω–∏—è
                not direct_text.startswith('[') and  # –ù–µ –∑–∞–ø–æ—á–≤–∞ —Å [price links]
                'See all newsletters' not in direct_text):
            meaningful_texts.append(direct_text)

    if meaningful_texts:
        # –í–∑–µ–º–∞–º–µ –Ω–∞–π-–¥—ä–ª–≥–∏—Ç–µ –∏ –Ω–∞–π-–∑–Ω–∞—á–∏–º–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤–µ
        meaningful_texts.sort(key=len, reverse=True)
        selected_texts = meaningful_texts[:5]  # –ü—ä—Ä–≤–∏—Ç–µ 5 –Ω–∞–π-–¥—ä–ª–≥–∏

        content = '\n\n'.join(selected_texts)
        if len(content) > 200:
            print(f"‚úÖ Div text extraction: {len(content)} chars")
            return content

    # –°–¢–†–ê–¢–ï–ì–ò–Ø 5: Fallback - body text
    print("üéØ –°–¢–†–ê–¢–ï–ì–ò–Ø 5: Body fallback...")
    body = soup.find('body')
    if body:
        # –ü—Ä–µ–º–∞—Ö–≤–∞–º–µ –Ω–µ–∂–µ–ª–∞–Ω–∏ –µ–ª–µ–º–µ–Ω—Ç–∏
        for unwanted in body(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            unwanted.decompose()

        body_text = body.get_text(separator=' ', strip=True)

        # –†–∞–∑–¥–µ–ª—è–º–µ –ø–æ –∏–∑—Ä–µ—á–µ–Ω–∏—è –∏ –≤–∑–µ–º–∞–º–µ —Å–º–∏—Å–ª–µ–Ω–∏—Ç–µ
        sentences = [s.strip() for s in body_text.split('.') if s.strip()]
        meaningful_sentences = []

        for sentence in sentences:
            if (20 < len(sentence) < 500 and
                    not sentence.startswith('[') and
                    'See all newsletters' not in sentence and
                    'Sign up' not in sentence):
                meaningful_sentences.append(sentence)

        if meaningful_sentences:
            # –í–∑–µ–º–∞–º–µ –ø—ä—Ä–≤–∏—Ç–µ 20 –∏–∑—Ä–µ—á–µ–Ω–∏—è
            content = '. '.join(meaningful_sentences[:20]) + '.'
            if len(content) > 200:
                print(f"‚úÖ Body fallback extraction: {len(content)} chars")
                return content

    print("‚ùå –í—Å–∏—á–∫–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–µ—É—Å–ø–µ—à–Ω–∏")
    return "–°—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–µ –º–æ–∂–µ –¥–∞ –±—ä–¥–µ –∏–∑–≤–ª–µ—á–µ–Ω–æ"


def _process_paragraphs_fixed(self, paragraphs):
    """–ü–æ–¥–æ–±—Ä–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏"""
    meaningful_paragraphs = []

    for p in paragraphs:
        text = p.get_text().strip()

        # –ü–æ-—Å—Ç—Ä–æ–≥–∞ —Ñ–∏–ª—Ç—Ä–∞—Ü–∏—è
        if self._is_meaningful_paragraph_fixed(text):
            meaningful_paragraphs.append(text)

    return '\n\n'.join(meaningful_paragraphs)


def _is_meaningful_paragraph_fixed(self, text):
    """–ü–æ–¥–æ–±—Ä–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞ —Å–º–∏—Å–ª–µ–Ω–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏"""

    # –û—Å–Ω–æ–≤–Ω–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏
    if len(text) < 20:
        return False

    # –ò–∑–∫–ª—é—á–≤–∞–º–µ price links –∏ navigation
    if text.startswith('[') and text.endswith(']'):
        return False

    # –ò–∑–∫–ª—é—á–≤–∞–º–µ –Ω–µ–∂–µ–ª–∞–Ω–∏ —Ñ—Ä–∞–∑–∏
    exclude_phrases = [
        'Sign up', 'Subscribe', 'Newsletter', 'See all newsletters',
        'Don\'t miss', 'By signing up', 'privacy policy', 'terms of use',
        'Cookie', 'Advertisement', 'Sponsored', 'Follow us', 'Share this',
        'Read more', 'Click here', 'Download', 'Watch', 'Listen',
        'Back to menu', 'What to know:', 'See more'
    ]


def _is_meaningful_paragraph_fixed(self, text):
    """–ü–æ–¥–æ–±—Ä–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞ —Å–º–∏—Å–ª–µ–Ω–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏"""

    # –û—Å–Ω–æ–≤–Ω–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏
    if len(text) < 20:
        return False

    # –ò–∑–∫–ª—é—á–≤–∞–º–µ price links –∏ navigation
    if text.startswith('[') and text.endswith(']'):
        return False

    # –ò–∑–∫–ª—é—á–≤–∞–º–µ –Ω–µ–∂–µ–ª–∞–Ω–∏ —Ñ—Ä–∞–∑–∏
    exclude_phrases = [
        'Sign up', 'Subscribe', 'Newsletter', 'See all newsletters',
        'Don\'t miss', 'By signing up', 'privacy policy', 'terms of use',
        'Cookie', 'Advertisement', 'Sponsored', 'Follow us', 'Share this',
        'Read more', 'Click here', 'Download', 'Watch', 'Listen',
        'Back to menu', 'What to know:', 'See more'
    ]

    text_lower = text.lower()
    for phrase in exclude_phrases:
        if phrase.lower() in text_lower:
            return False

    # –¢—Ä—è–±–≤–∞ –¥–∞ –∏–º–∞ –ø–æ–Ω–µ –µ–¥–Ω–æ –∏–∑—Ä–µ—á–µ–Ω–∏–µ
    if text.count('.') < 1 and text.count('!') < 1 and text.count('?') < 1:
        return False

    # –ù–µ —Ç—Ä—è–±–≤–∞ –¥–∞ –µ —Å–∞–º–æ —Ü–∏—Ñ—Ä–∏ –∏–ª–∏ –∫—Ä–∞—Ç–∫–∏ —Ñ—Ä–∞–∑–∏
    words = text.split()
    if len(words) < 5:
        return False

    return True


from bs4 import BeautifulSoup
import time
from datetime import datetime, timedelta
from urllib.parse import urljoin
import re
import json

from config import (
    COINDESK_BASE_URL,
    REQUEST_HEADERS,
    SCRAPING_CONFIG,
    HTML_SELECTORS
)
from database import DatabaseManager


class CoinDeskLatestNewsScraper:
    def __init__(self, use_database=True):
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ CoinDesk Latest News Scraper...")
        self.session = requests.Session()

        # Headers
        simple_headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
        }
        self.session.headers.update(simple_headers)

        # URL –∑–∞ latest news
        self.latest_news_url = "https://www.coindesk.com/latest-crypto-news"

        # Database –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
        self.use_database = use_database
        if use_database:
            self.db = DatabaseManager()
        else:
            self.db = None

        print("‚úÖ Latest News Scraper –≥–æ—Ç–æ–≤!")

    def get_articles_by_date_filter(self, date_filter='today', max_articles=50):
        """
        –ü–æ–ª—É—á–∞–≤–∞ —Å—Ç–∞—Ç–∏–∏ –æ—Ç latest-crypto-news —Å date —Ñ–∏–ª—Ç—ä—Ä

        date_filter –º–æ–∂–µ –¥–∞ –µ:
        - 'today' - —Å–∞–º–æ –¥–Ω–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏
        - 'yesterday' - –≤—á–µ—Ä–∞—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏
        - '2025-06-10' - –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –¥–∞—Ç–∞
        - 'last_3_days' - –ø–æ—Å–ª–µ–¥–Ω–∏—Ç–µ 3 –¥–Ω–∏
        - 'all' - –≤—Å–∏—á–∫–∏ (–¥–æ max_articles)
        """
        print(f"üîç –¢—ä—Ä—Å–µ–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏–∏ —Å —Ñ–∏–ª—Ç—ä—Ä: {date_filter}")

        # –û–ø—Ä–µ–¥–µ–ª—è–º–µ target –¥–∞—Ç–∏
        target_dates = self._get_target_dates(date_filter)
        print(f"üìÖ Target –¥–∞—Ç–∏: {target_dates}")

        # –ó–∞–ø–æ—á–≤–∞–º–µ –¥–∞ —Å–∫—Ä–∞–ø–≤–∞–º–µ —Å—Ç—Ä–∞–Ω–∏—Ü–∏
        all_articles = []
        page_offset = 0
        pages_checked = 0
        max_pages = 10  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç

        while len(all_articles) < max_articles and pages_checked < max_pages:
            print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {pages_checked + 1}...")

            # –°–∫—Ä–∞–ø–≤–∞–º–µ —Ç–µ–∫—É—â–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞
            page_articles = self._scrape_latest_news_page(page_offset)

            if not page_articles:
                print("‚ùå –ù—è–º–∞ –ø–æ–≤–µ—á–µ —Å—Ç–∞—Ç–∏–∏")
                break

            # –§–∏–ª—Ç—Ä–∏—Ä–∞–º–µ –ø–æ –¥–∞—Ç–∞
            filtered_articles = []
            for article in page_articles:
                article_date = self._extract_date_from_article_data(article)
                if article_date in target_dates or date_filter == 'all':
                    filtered_articles.append(article)
                elif date_filter != 'all' and article_date < min(target_dates):
                    # –ê–∫–æ —Å—Ç–∞—Ç–∏—è—Ç–∞ –µ –ø–æ-—Å—Ç–∞—Ä–∞ –æ—Ç –Ω–∞–π-—Å—Ç–∞—Ä–∞—Ç–∞ target –¥–∞—Ç–∞, —Å–ø–∏—Ä–∞–º–µ
                    print(f"‚èπÔ∏è –î–æ—Å—Ç–∏–≥–Ω–∞—Ö–º–µ —Å—Ç–∞—Ä–∏ —Å—Ç–∞—Ç–∏–∏ ({article_date}), —Å–ø–∏—Ä–∞–º–µ")
                    return all_articles[:max_articles]

            all_articles.extend(filtered_articles)
            pages_checked += 1
            page_offset += 16  # CoinDesk –ø–æ–∫–∞–∑–≤–∞ 16 —Å—Ç–∞—Ç–∏–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞

            print(f"üìä –°—Ç—Ä–∞–Ω–∏—Ü–∞ {pages_checked}: {len(filtered_articles)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏ —Å—Ç–∞—Ç–∏–∏")

            # –ú–∞–ª–∫–∞ –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∏
            time.sleep(2)

        print(f"‚úÖ –ù–∞–º–µ—Ä–µ–Ω–∏ {len(all_articles)} —Å—Ç–∞—Ç–∏–∏ —Å —Ñ–∏–ª—Ç—ä—Ä '{date_filter}'")
        return all_articles[:max_articles]

    def _get_target_dates(self, date_filter):
        """–í—Ä—ä—â–∞ —Å–ø–∏—Å—ä–∫ —Å target –¥–∞—Ç–∏ –∑–∞ —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ"""
        today = datetime.now().date()

        if date_filter == 'today':
            return [today]
        elif date_filter == 'yesterday':
            yesterday = today - timedelta(days=1)
            return [yesterday]
        elif date_filter == 'last_3_days':
            return [today - timedelta(days=i) for i in range(3)]
        elif date_filter == 'last_week':
            return [today - timedelta(days=i) for i in range(7)]
        elif isinstance(date_filter, str) and re.match(r'\d{4}-\d{2}-\d{2}', date_filter):
            # –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç YYYY-MM-DD
            target_date = datetime.strptime(date_filter, '%Y-%m-%d').date()
            return [target_date]
        else:
            # 'all' –∏–ª–∏ –Ω–µ—Ä–∞–∑–ø–æ–∑–Ω–∞—Ç —Ñ–∏–ª—Ç—ä—Ä
            return []

    def _scrape_latest_news_page(self, offset=0):
        """–°–∫—Ä–∞–ø–≤–∞ –µ–¥–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ—Ç latest-crypto-news"""
        try:
            # URL –∑–∞ pagination –º–æ–∂–µ –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞ offset –ø–∞—Ä–∞–º–µ—Ç—ä—Ä
            url = f"{self.latest_news_url}?offset={offset}" if offset > 0 else self.latest_news_url

            response = self.session.get(url, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # –¢—ä—Ä—Å–∏–º —Å—Ç–∞—Ç–∏–∏ - –æ–±–∏–∫–Ω–æ–≤–µ–Ω–æ —Å–∞ –≤ article elements –∏–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏ containers
            articles = []

            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –¢—ä—Ä—Å–∏–º article elements
            article_elements = soup.find_all('article')
            print(f"üîç –ù–∞–º–µ—Ä–µ–Ω–∏ {len(article_elements)} article elements")

            for article_elem in article_elements:
                article_data = self._extract_article_data_from_element(article_elem)
                if article_data:
                    articles.append(article_data)

            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ê–∫–æ –Ω—è–º–∞ article elements, —Ç—ä—Ä—Å–∏–º –ø–æ –ª–∏–Ω–∫–æ–≤–µ
            if not articles:
                print("üîç –¢—ä—Ä—Å—è —Å—Ç–∞—Ç–∏–∏ –ø–æ –ª–∏–Ω–∫–æ–≤–µ...")
                link_elements = soup.find_all('a', href=True)
                for link in link_elements:
                    href = link['href']
                    if self._is_valid_article_url(href):
                        article_data = {
                            'url': self._make_full_url(href),
                            'title': link.get_text().strip(),
                            'href': href
                        }
                        if article_data['title'] and len(article_data['title']) > 15:
                            articles.append(article_data)

            return articles[:16]  # CoinDesk –ø–æ–∫–∞–∑–≤–∞ 16 –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞

        except Exception as e:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ scraping –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {e}")
            return []

    def _extract_article_data_from_element(self, article_elem):
        """–ò–∑–≤–ª–∏—á–∞ –¥–∞–Ω–Ω–∏ –∑–∞ —Å—Ç–∞—Ç–∏—è –æ—Ç article element"""
        try:
            # –¢—ä—Ä—Å–∏–º –ª–∏–Ω–∫ –≤ article element
            link = article_elem.find('a', href=True)
            if not link:
                return None

            href = link['href']
            if not self._is_valid_article_url(href):
                return None

            # –ò–∑–≤–ª–∏—á–∞–º–µ –∑–∞–≥–ª–∞–≤–∏–µ—Ç–æ
            title = ""
            # –¢—ä—Ä—Å–∏–º h1, h2, h3 –≤ article
            for header_tag in ['h1', 'h2', 'h3']:
                header = article_elem.find(header_tag)
                if header:
                    title = header.get_text().strip()
                    break

            # –ê–∫–æ –Ω—è–º–∞ header, –≤–∑–µ–º–∞–º–µ –æ—Ç link text
            if not title:
                title = link.get_text().strip()

            if not title or len(title) < 15:
                return None

            return {
                'url': self._make_full_url(href),
                'title': title,
                'href': href
            }

        except Exception as e:
            print(f"‚ö†Ô∏è –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ article data: {e}")
            return None

    def _is_valid_article_url(self, href):
        """–ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ URL –µ –≤–∞–ª–∏–¥–µ–Ω –∑–∞ —Å—Ç–∞—Ç–∏—è"""
        if not href or href in ['#', '/', '']:
            return False

        if href.startswith('http') and 'coindesk.com' not in href:
            return False

        if href.startswith('#') or href.startswith('mailto:') or href.startswith('tel:'):
            return False

        # –ò–∑–∫–ª—é—á–≤–∞–º–µ —Å–∏—Å—Ç–µ–º–Ω–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∏
        exclude_patterns = [
            '/newsletters/', '/podcasts/', '/events/', '/about/', '/careers/',
            '/advertise/', '/price/', '/author/', '/tag/', '/sponsored-content/',
            '/_next/', '/api/', '/search', '/privacy', '/terms'
        ]

        for pattern in exclude_patterns:
            if pattern in href:
                return False

        # –ü—Ä–∏–µ–º–∞–º–µ —Å—Ç–∞—Ç–∏–∏ —Å –¥–∞—Ç–∞ –∏–ª–∏ –æ—Ç news –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        date_patterns = [
            r'/\d{4}/\d{2}/\d{2}/',  # /2025/06/10/
            r'/2025/', r'/2024/'
        ]

        has_date = any(re.search(pattern, href) for pattern in date_patterns)

        news_categories = ['/markets/', '/policy/', '/tech/', '/business/', '/layer2/', '/web3/', '/daybook']
        has_category = any(category in href for category in news_categories)

        return has_date or has_category

    def _make_full_url(self, href):
        """–ü—Ä–∞–≤–∏ –ø—ä–ª–µ–Ω URL –æ—Ç relative href"""
        if href.startswith('http'):
            return href
        elif href.startswith('/'):
            return f"https://www.coindesk.com{href}"
        else:
            return f"https://www.coindesk.com/{href}"

    def _extract_date_from_article_data(self, article_data):
        """–ò–∑–≤–ª–∏—á–∞ –¥–∞—Ç–∞ –æ—Ç article data"""
        url = article_data['url']

        # –û–ø–∏—Ç–≤–∞–º–µ –¥–∞ –∏–∑–≤–ª–µ—á–µ–º –æ—Ç URL
        date_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', url)
        if date_match:
            year, month, day = date_match.groups()
            try:
                return datetime(int(year), int(month), int(day)).date()
            except:
                pass

        # –ê–∫–æ –Ω—è–º–∞ –¥–∞—Ç–∞ –≤ URL, –ø—Ä–∏–µ–º–∞–º–µ —á–µ –µ –æ—Ç –¥–Ω–µ—Å–∫–∞ (latest news)
        return datetime.now().date()

    def scrape_articles_smart(self, date_filter='today', limit=10, save_to_db=True):
        """
        Smart scraping —Å date —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ

        date_filter:
        - 'today' - —Å–∞–º–æ –¥–Ω–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏
        - 'yesterday' - –≤—á–µ—Ä–∞—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏
        - '2025-06-10' - –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –¥–∞—Ç–∞
        - 'last_3_days' - –ø–æ—Å–ª–µ–¥–Ω–∏—Ç–µ 3 –¥–Ω–∏
        """
        print(f"üéØ Smart scraping: {limit} —Å—Ç–∞—Ç–∏–∏ —Å —Ñ–∏–ª—Ç—ä—Ä '{date_filter}'")

        # –í–∑–µ–º–∞–º–µ —Å—Ç–∞—Ç–∏–∏—Ç–µ —Å —Ñ–∏–ª—Ç—ä—Ä
        article_links = self.get_articles_by_date_filter(date_filter, max_articles=limit * 2)

        if not article_links:
            print("‚ùå –ù–µ —Å–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —Å—Ç–∞—Ç–∏–∏ —Å —Ç–æ–∑–∏ —Ñ–∏–ª—Ç—ä—Ä")
            return []

        # Database —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ
        if self.db and save_to_db:
            print("üîç –ü—Ä–æ–≤–µ—Ä—è–≤–∞–Ω–µ –∑–∞ –¥—É–±–ª–∏—Ä–∞—â–∏ —Å–µ URLs...")
            new_article_links = []
            skipped_count = 0

            for link_info in article_links:
                url = link_info['url']
                if not self.db.is_url_scraped_before(url):
                    new_article_links.append(link_info)
                else:
                    skipped_count += 1
                    self.db.record_scraped_url(url)

            print(f"üìä {len(new_article_links)} –Ω–æ–≤–∏ —Å—Ç–∞—Ç–∏–∏, {skipped_count} –≤–µ—á–µ scraped")
            article_links = new_article_links

        # –û–≥—Ä–∞–Ω–∏—á–∞–≤–∞–º–µ –¥–æ –ª–∏–º–∏—Ç–∞
        article_links = article_links[:limit]

        if not article_links:
            print("‚ÑπÔ∏è –í—Å–∏—á–∫–∏ —Å—Ç–∞—Ç–∏–∏ —Å–∞ –≤–µ—á–µ scraped")
            return []

        # Scraping –Ω–∞ —Å—Ç–∞—Ç–∏–∏—Ç–µ
        scraped_articles = []
        successful_count = 0
        failed_count = 0

        for i, link_info in enumerate(article_links, 1):
            url = link_info['url']
            print(f"\n[{i}/{len(article_links)}] {link_info['title'][:60]}...")

            article_data = self.scrape_single_article(url)
            if article_data:
                scraped_articles.append(article_data)
                successful_count += 1

                if self.db and save_to_db:
                    self.db.save_article(article_data)
            else:
                failed_count += 1

        print(f"\nüéâ Smart scraping –∑–∞–≤—ä—Ä—à–µ–Ω!")
        print(f"üìä –†–µ–∑—É–ª—Ç–∞—Ç: {successful_count} —É—Å–ø–µ—à–Ω–∏, {failed_count} –Ω–µ—É—Å–ø–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏")

        return scraped_articles

    def scrape_single_article(self, article_url):
        """–ò–∑–≤–ª–∏—á–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–∞ –µ–¥–Ω–∞ —Å—Ç–∞—Ç–∏—è (–∏–∑–ø–æ–ª–∑–≤–∞ —Å—ä—â–∞—Ç–∞ –ª–æ–≥–∏–∫–∞ –∫–∞—Ç–æ —Å—Ç–∞—Ä–∏—è scraper)"""
        print(f"üìÑ Scraping —Å—Ç–∞—Ç–∏—è: {article_url}")

        try:
            time.sleep(SCRAPING_CONFIG['delay_between_requests'])

            response = self.session.get(article_url, timeout=15)
            response.raise_for_status()

            content_text = response.content.decode('utf-8', errors='ignore')
            soup = BeautifulSoup(content_text, 'html.parser')

            # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ —Å—ä—â–∏—Ç–µ –º–µ—Ç–æ–¥–∏ –∑–∞ –∏–∑–≤–ª–∏—á–∞–Ω–µ –∫–∞—Ç–æ –≤ —Å—Ç–∞—Ä–∏—è scraper
            title = self._extract_title_improved(soup)
            content = self._extract_content_improved(soup)
            date = self._extract_date_improved(soup)
            author = self._extract_author_improved(soup)

            if len(content) < SCRAPING_CONFIG['min_article_length']:
                print(f"‚ö†Ô∏è  –°—Ç–∞—Ç–∏—è—Ç–∞ –µ —Ç–≤—ä—Ä–¥–µ –∫—Ä–∞—Ç–∫–∞ ({len(content)} chars)")
                return None

            article_data = {
                'url': article_url,
                'title': title,
                'content': content,
                'date': date,
                'author': author,
                'scraped_at': datetime.now(),
                'content_length': len(content)
            }

            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—è: {title[:50]}... ({len(content)} chars)")
            return article_data

        except Exception as e:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ scraping –Ω–∞ {article_url}: {str(e)}")
            return None

    # –°—ä—â–∏—Ç–µ –º–µ—Ç–æ–¥–∏ –∑–∞ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ –∫–∞—Ç–æ –≤ —Å—Ç–∞—Ä–∏—è scraper
    def _extract_title_improved(self, soup):
        h1_tags = soup.find_all('h1')
        for h1 in h1_tags:
            text = h1.get_text().strip()
            if text and len(text) > 10:
                return text

        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title['content'].strip()

        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
            title = re.sub(r'\s*\|\s*CoinDesk.*$', '', title)
            if title:
                return title
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –∑–∞–≥–ª–∞–≤–∏–µ"

    def _extract_content_improved(self, soup):
        print("üîç –ó–∞–ø–æ—á–≤–∞–º –ø–æ–¥–æ–±—Ä–µ–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ...")

        # –°–¢–†–ê–¢–ï–ì–ò–Ø 1: Main containers
        main_selectors = [
            'main',
            'article',
            '[role="main"]',
            '.article-content',
            '.post-content',
            '.entry-content'
        ]

        for selector in main_selectors:
            container = soup.select_one(selector)
            if container:
                print(f"‚úÖ –ù–∞–º–µ—Ä–µ–Ω main container: {selector}")
                paragraphs = container.find_all('p')
                content = self._process_paragraphs(paragraphs)
                if len(content) > 200:
                    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏ {len(content)} chars –æ—Ç {selector}")
                    return content

        # –°–¢–†–ê–¢–ï–ì–ò–Ø 2: –í—Å–∏—á–∫–∏ <p> tags –Ω–æ —Å –ø–æ-—É–º–Ω–∞ —Ñ–∏–ª—Ç—Ä–∞—Ü–∏—è
        print("üîç –¢—ä—Ä—Å—è –≤—Å–∏—á–∫–∏ <p> tags...")
        all_paragraphs = soup.find_all('p')
        print(f"üìä –ù–∞–º–µ—Ä–µ–Ω–∏ {len(all_paragraphs)} –æ–±—â–æ <p> tags")

        if all_paragraphs:
            content = self._process_paragraphs(all_paragraphs)
            if len(content) > 100:
                print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏ {len(content)} chars –æ—Ç –≤—Å–∏—á–∫–∏ <p> tags")
                return content

        # –°–¢–†–ê–¢–ï–ì–ò–Ø 3: Div containers —Å —Ç–µ–∫—Å—Ç
        print("üîç –¢—ä—Ä—Å—è div containers —Å —Ç–µ–∫—Å—Ç...")
        text_divs = soup.find_all('div')
        meaningful_text = []

        for div in text_divs:
            # –í–∑–µ–º–∞–º–µ —Å–∞–º–æ direct text, –Ω–µ nested elements
            direct_text = div.get_text().strip()
            if 50 < len(direct_text) < 1000:  # –†–∞–∑—É–º–Ω–∞ –¥—ä–ª–∂–∏–Ω–∞
                meaningful_text.append(direct_text)

        if meaningful_text:
            content = '\n\n'.join(meaningful_text[:10])  # –í–∑–µ–º–∞–º–µ –ø—ä—Ä–≤–∏—Ç–µ 10
            if len(content) > 100:
                print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏ {len(content)} chars –æ—Ç div containers")
                return content

        # –°–¢–†–ê–¢–ï–ì–ò–Ø 4: Fallback - –≤—Å–∏—á–∫–æ –≤ body
        print("üîç Fallback: –í–∑–µ–º–∞–º –≤—Å–∏—á–∫–æ –æ—Ç body...")
        body = soup.find('body')
        if body:
            # –ü—Ä–µ–º–∞—Ö–≤–∞–º–µ script –∏ style tags
            for script in body(["script", "style", "nav", "header", "footer"]):
                script.decompose()

            body_text = body.get_text()
            # –ü–æ—á–∏—Å—Ç–≤–∞–º–µ –∏ –≤–∑–µ–º–∞–º–µ —Ä–∞–∑—É–º–Ω–∞ —á–∞—Å—Ç
            lines = [line.strip() for line in body_text.split('\n') if line.strip()]
            content = '\n'.join(lines[:50])  # –ü—ä—Ä–≤–∏—Ç–µ 50 —Ä–µ–¥–∞

            if len(content) > 100:
                print(f"‚úÖ Fallback –∏–∑–≤–ª–µ—á–µ–Ω–∏ {len(content)} chars –æ—Ç body")
                return content

        print("‚ùå –ù–µ —É—Å–ø—è—Ö –¥–∞ –∏–∑–≤–ª–µ–∫–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ")
        return "–°—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–µ –º–æ–∂–µ –¥–∞ –±—ä–¥–µ –∏–∑–≤–ª–µ—á–µ–Ω–æ"

    def _process_paragraphs(self, paragraphs):
        meaningful_paragraphs = []
        for p in paragraphs:
            text = p.get_text().strip()
            if self._is_meaningful_paragraph(text):
                meaningful_paragraphs.append(text)
        return '\n\n'.join(meaningful_paragraphs)

    def _is_meaningful_paragraph(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –µ —Å–º–∏—Å–ª–µ–Ω"""
        if len(text) < 20:  # –¢–≤—ä—Ä–¥–µ –∫—Ä–∞—Ç—ä–∫
            return False

        # –ò–∑–∫–ª—é—á–≤–∞–º–µ –Ω–µ–∂–µ–ª–∞–Ω–∏ —Ñ—Ä–∞–∑–∏
        exclude_phrases = [
            'Sign up', 'Subscribe', 'Newsletter', 'See all newsletters',
            'Don\'t miss', 'By signing up', 'privacy policy', 'terms of use',
            'Cookie', 'Advertisement', 'Sponsored', 'Follow us', 'Share this',
            'Read more', 'Click here', 'Download', 'Watch', 'Listen'
        ]

        text_lower = text.lower()
        for phrase in exclude_phrases:
            if phrase.lower() in text_lower:
                return False

        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –∑–∞ –Ω–æ—Ä–º–∞–ª–Ω–∏ –∏–∑—Ä–µ—á–µ–Ω–∏—è
        if text.count('.') < 1:  # –ù—è–º–∞ –∏–∑—Ä–µ—á–µ–Ω–∏—è
            return False

        return True

    def _extract_date_improved(self, soup):
        published_meta = soup.find('meta', property='article:published_time')
        if published_meta and published_meta.get('content'):
            try:
                date_str = published_meta['content']
                return datetime.fromisoformat(date_str.replace('Z', '+00:00')).strftime('%Y-%m-%d')
            except:
                pass
        return datetime.now().strftime('%Y-%m-%d')

    def _extract_author_improved(self, soup):
        author_meta = soup.find('meta', attrs={'name': 'author'})
        if author_meta and author_meta.get('content'):
            return author_meta['content'].strip()
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω –∞–≤—Ç–æ—Ä"

    def get_scraping_status_by_date(self, date_str):
        """–ü–æ–∫–∞–∑–≤–∞ —Å—Ç–∞—Ç—É—Å –Ω–∞ scraping –∑–∞ –¥–∞–¥–µ–Ω–∞ –¥–∞—Ç–∞"""
        if not self.db:
            return {'error': 'Database –Ω–µ –µ –∞–∫—Ç–∏–≤–Ω–∞'}

        try:
            conn = self.db.get_connection() if hasattr(self.db, 'get_connection') else sqlite3.connect(self.db.db_path)
            cursor = conn.cursor()

            # –°—Ç–∞—Ç–∏–∏ –æ—Ç —Ç–∞–∑–∏ –¥–∞—Ç–∞
            cursor.execute("""
                SELECT COUNT(*) FROM articles 
                WHERE url LIKE ? OR published_date = ?
            """, (f'%{date_str.replace("-", "/")}%', date_str))

            scraped_count = cursor.fetchone()[0]

            # –ù–∞–º–∏—Ä–∞–º–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–Ω–∏ —Å—Ç–∞—Ç–∏–∏ –æ—Ç latest news
            potential_articles = self.get_articles_by_date_filter(date_str, max_articles=50)
            potential_count = len(potential_articles)

            new_count = 0
            for article in potential_articles:
                cursor.execute("SELECT 1 FROM scraped_urls WHERE url = ?", (article['url'],))
                if not cursor.fetchone():
                    new_count += 1

            conn.close()

            return {
                'date': date_str,
                'scraped_articles': scraped_count,
                'potential_articles': potential_count,
                'new_to_scrape': new_count,
                'completion_rate': f"{(scraped_count / potential_count * 100):.1f}%" if potential_count > 0 else "N/A"
            }

        except Exception as e:
            return {'error': str(e)}


# –¢–µ—Å—Ç–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏
def test_latest_news_scraper():
    """–¢–µ—Å—Ç–≤–∞ –Ω–æ–≤–∏—è latest news scraper"""
    print("=== –¢–ï–°–¢ –ù–ê LATEST NEWS SCRAPER ===")

    scraper = CoinDeskLatestNewsScraper(use_database=False)

    # –¢–µ—Å—Ç 1: –î–Ω–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏
    print("\n1. –¢–µ—Å—Ç: –î–Ω–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏")
    today_articles = scraper.get_articles_by_date_filter('today', max_articles=10)
    print(f"üìä –ù–∞–º–µ—Ä–µ–Ω–∏ {len(today_articles)} –¥–Ω–µ—à–Ω–∏ —Å—Ç–∞—Ç–∏–∏")

    for i, article in enumerate(today_articles[:3], 1):
        print(f"   {i}. {article['title'][:60]}...")

    # –¢–µ—Å—Ç 2: Smart scraping
    print("\n2. –¢–µ—Å—Ç: Smart scraping –Ω–∞ 3 —Å—Ç–∞—Ç–∏–∏")
    scraped = scraper.scrape_articles_smart('today', limit=3, save_to_db=False)
    print(f"üìä –£—Å–ø–µ—à–Ω–æ scraped: {len(scraped)} —Å—Ç–∞—Ç–∏–∏")

    return len(scraped) > 0


if __name__ == "__main__":
    success = test_latest_news_scraper()
    if success:
        print("\n‚úÖ Latest News Scraper —Ä–∞–±–æ—Ç–∏ –æ—Ç–ª–∏—á–Ω–æ!")
    else:
        print("\n‚ùå –ò–º–∞ –ø—Ä–æ–±–ª–µ–º–∏ —Å Latest News Scraper")
